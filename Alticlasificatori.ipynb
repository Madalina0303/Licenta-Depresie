{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1wly9gG6PWFlzoznEwGaNGW7MIO2qZDjD",
      "authorship_tag": "ABX9TyM4fp3DIuwoya2C8dm1arwN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madalina0303/Licenta-Depresie/blob/main/Alticlasificatori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qA8eHtj4SOLF"
      },
      "outputs": [],
      "source": [
        "from xml.dom.minidom import parse\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
        "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
        "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
        "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
        "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
        "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
        "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
        "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
        "                     \"i'd\": \"i would\", \"i'd've\": \"i would have\",\"i'll\": \"i will\",\n",
        "                     \"i'll've\": \"i will have\",\"i'm\": \"i am\",\"I've\": \"i have\", \"isn't\": \"is not\",\n",
        "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
        "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
        "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
        "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
        "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
        "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
        "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
        "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
        "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
        "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
        "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
        "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
        "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
        "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
        "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
        "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
        "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
        "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
        "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
        "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
        "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
        "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
        "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
        "                     \"you've\": \"you have\"}"
      ],
      "metadata": {
        "id": "jTLM2LZ3Xqdr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)"
      ],
      "metadata": {
        "id": "YaYAtrNjX6I8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADrMdCHReaIr",
        "outputId": "56b04d85-9929-46d0-b42f-64a537da3bfd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.add('subject')\n",
        "stop_words.add('http')\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n"
      ],
      "metadata": {
        "id": "3wemFxPObfTW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])"
      ],
      "metadata": {
        "id": "QcUkbI5wdNCV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_file(filename):\n",
        "    doc = parse(filename, )\n",
        "    titles = doc.getElementsByTagName(\"TITLE\")\n",
        "    posts = doc.getElementsByTagName(\"TEXT\")\n",
        "    messages = []\n",
        "    for index, title in enumerate(titles):\n",
        "        message = title.firstChild.nodeValue + posts[index].firstChild.nodeValue\n",
        "        message = expand_contractions(message).lower()\n",
        "        message= re.sub('[%s]' % re.escape(string.punctuation), '' , message)\n",
        "        message = re.sub('W*dw*','',message)\n",
        "        message = remove_stopwords(message)\n",
        "        message = re.sub('(http[s]?S+)|(w+.[A-Za-z]{2,4}S*)', '', message)\n",
        "        message = lemmatize_words(message)\n",
        "        message  = re.sub(' +', ' ', message)\n",
        "        messages.append(message.strip())\n",
        "        \n",
        "    return messages\n",
        "\n",
        "def get_data_post(dir_name):\n",
        "    users_posts = dict()\n",
        "    for root, dir, files in os.walk(dir_name):\n",
        "        for filename in files:\n",
        "            if \".xml\" in filename:\n",
        "                nr = re.findall(\"[0-9]+\", filename)\n",
        "                users_posts[nr[0]] = parse_file(root+\"/\"+filename)\n",
        "\n",
        "    return users_posts"
      ],
      "metadata": {
        "id": "TFgtdgXdSxot"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " posts = get_data_post(\"drive/MyDrive/data\")\n",
        "#  print(posts)"
      ],
      "metadata": {
        "id": "SJm15b-DS5XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "r8YexraLqFpQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dam append sa facem un vector mare din toate postarile, oare cum ar fi cu o matrice ??"
      ],
      "metadata": {
        "id": "ccLhIekjvW-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_post_list(dir_name):\n",
        "    users_posts_list = []\n",
        "    for root, dir, files in os.walk(dir_name):\n",
        "        for filename in files:\n",
        "            if \".xml\" in filename:\n",
        "                nr = re.findall(\"[0-9]+\", filename)\n",
        "                users_posts_list.append(parse_file(root+\"/\"+filename))\n",
        "\n",
        "    return users_posts_list"
      ],
      "metadata": {
        "id": "TnP7YlFvVmC4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = get_data_post_list(\"drive/MyDrive/data\")\n",
        "x_train = all_data[:60]\n",
        "x_test = all_data[60:]\n"
      ],
      "metadata": {
        "id": "gfPDNjMZYi_j"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = 1000)\n",
        " # Stop words are common words in English that don't tell us anything about the polarity of a review.\n",
        "    # Such words include the, that, and a\n",
        "# Converts a collection of text documents to a matrix of token counts\n",
        "# max_features = maximum number of words we'd like to have in our bag of words model\n",
        "X = []\n",
        "for i in all_data:\n",
        "  X.append(cv.fit_transform(i).toarray())"
      ],
      "metadata": {
        "id": "gm0IyL2vsdGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.get_feature_names()"
      ],
      "metadata": {
        "id": "350VGLQLt2zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tf_transformer = TfidfTransformer()\n",
        "X = tf_transformer.fit_transform(X).toarray()"
      ],
      "metadata": {
        "id": "7QKRGkyYu3qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidfVectorizer = TfidfVectorizer(max_features =1000)\n",
        "X = []\n",
        "for i in all_data:\n",
        "  X.append(tfidfVectorizer.fit_transform(i).toarray())\n",
        "  \n"
      ],
      "metadata": {
        "id": "Z4VFRMLi4jS5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 5000) \n",
        "for i in all_data:\n",
        "  vectorizer.fit(i) \n",
        "  X.append(tfidfVectorizer.transform(i).toarray())\n",
        "\n",
        "# Xtrain = vectorizer.transform(all_data)\n",
        "print(len(X[0]))\n",
        "print(len(X[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGe5bt2k-BYd",
        "outputId": "5b091206-d8cc-4e55-dc97-e2c5de64b289"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73\n",
            "43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X[1]))\n",
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuQFu8-v5IKJ",
        "outputId": "8f1db77b-c346-4e3f-992a-e32bb0821416"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.0.6-cp37-none-manylinux1_x86_64.whl (76.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 76.6 MB 103 kB/s \n",
            "\u001b[?25hRequirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (4.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (8.0.1)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPd9acJF9Q9y",
        "outputId": "4702a6ae-cb95-40f2-a133-b2f57847e915"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.36931339 0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.18000993 0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Naive Bayes is a statistical classification technique based on Bayes Theorem\n",
        "# common classifier used in sentiment analysis is the Naive Bayes Classifier.\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier # this is experimental\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "classifiers = [GradientBoostingClassifier(),GaussianNB(),HistGradientBoostingClassifier(),\n",
        "               RandomForestClassifier(),LogisticRegression(),XGBClassifier(),LGBMClassifier(),\n",
        "               CatBoostClassifier(verbose=0)]"
      ],
      "metadata": {
        "id": "-2eZarX-5YGF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('drive/MyDrive/Licenta/reponses.json') as json_file:\n",
        "    responses = json.load(json_file)\n",
        "  \n",
        "y_train = responses[\"Sadness\"]\n",
        "import numpy as np\n",
        "# one hot target \n",
        "# momentan avem 4 raspunsuri de la 0 la 3 facem cu I4 la celelalte mai speciale facem cu I8\n",
        "print(y_train)\n",
        "# trg = [[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,1.0,0.0],[0.0,0.0,0.0,1.0]]\n",
        "# y_train_target = []\n",
        "# for j in y_train:\n",
        "#   y_train_target.append(trg[j])\n",
        "# y_1 = np.array(y_train_target)\n",
        "# print(type(y_train_target[0][0]))\n",
        "\n",
        "y_test = y_train[60:]\n",
        "y_train = y_train[0:60]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOch9H7C62kl",
        "outputId": "a445a20e-a640-458d-a124-a93ecee5325d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 2, 0, 1, 0, 1, 2, 1, 0, 1, 1, 1, 0, 0, 2, 1, 2, 1, 1, 0, 1, 0, 3, 1, 1, 1, 0, 3, 2, 1, 1, 3, 1, 1, 1, 0, 1, 1, 3, 1, 3, 2, 1, 1, 1, 1, 1, 0, 1, 2, 1, 2, 2, 1, 1, 3, 1, 0, 1, 1, 2, 2, 0, 1, 0, 0, 3, 1, 1, 0, 1, 1, 1, 3, 2, 3, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
        "for classifier in classifiers:\n",
        "    classifier.fit(x_train,y_train)\n",
        "    print(f'The {classifier}  Accuracy  is {accuracy_score(y_test,classifier.predict(X)) }' )"
      ],
      "metadata": {
        "id": "jAWMPO-z5lKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Acesta are format de cod\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "umWrdTUVsN0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "vocab_size = 100000\n",
        "oov_token = \"<OOV>\"\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(x_train)"
      ],
      "metadata": {
        "id": "ks_uAhOiage_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "x_test_sequences = tokenizer.texts_to_sequences(x_test)\n"
      ],
      "metadata": {
        "id": "HvEbwiaEauB7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n"
      ],
      "metadata": {
        "id": "XsnPY8QIclYo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_length = 150530\n",
        "padding_type = \"post\"\n",
        "trunction_type=\"post\"\n",
        "x_train_padded = pad_sequences(x_train_sequences, padding=padding_type,\n",
        "                       truncating=trunction_type,maxlen = 1218)\n",
        "x_test_padded = pad_sequences(x_test_sequences,\n",
        "                               padding=padding_type, truncating=trunction_type,maxlen = 1218)"
      ],
      "metadata": {
        "id": "nZUw_ZO4bBAs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
        "for classifier in classifiers:\n",
        "    classifier.fit(x_train_padded,y_train)\n",
        "    print(f'The {classifier}  Accuracy  is {accuracy_score(y_test,classifier.predict(x_test_padded )) }' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNxmRil5Cx6P",
        "outputId": "5bef13d3-f576-4b5c-96d7-ef57ced98818"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The GradientBoostingClassifier()  Accuracy  is 0.45\n",
            "The GaussianNB()  Accuracy  is 0.05\n",
            "The HistGradientBoostingClassifier()  Accuracy  is 0.45\n",
            "The RandomForestClassifier()  Accuracy  is 0.45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The LogisticRegression()  Accuracy  is 0.2\n",
            "The XGBClassifier(objective='multi:softprob')  Accuracy  is 0.45\n",
            "The LGBMClassifier()  Accuracy  is 0.45\n",
            "The <catboost.core.CatBoostClassifier object at 0x7f619f5fe550>  Accuracy  is 0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(x_train_padded))\n",
        "print(x_train_padded.shape)\n",
        "print(x_test_padded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrzqt63cYPH9",
        "outputId": "88475d1c-fbab-4c46-edca-f968fc9d6e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(60, 1218)\n",
            "(20, 1218)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "http://nlp.stanford.edu/data/glove.6B.zip \\\n",
        "-O /tmp/glove.6B.zip"
      ],
      "metadata": {
        "id": "CK0_mdKWemLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "with zipfile.ZipFile('/tmp/glove.6B.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/tmp/glove')"
      ],
      "metadata": {
        "id": "PqrLW6vBfdpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lg = 0\n",
        "import numpy as np\n",
        "embeddings_index = {}\n",
        "f = open('/tmp/glove/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    if len(coefs>lg):\n",
        "      lg = len(coefs)\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print(lg)\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "id": "1qYl-2dMfpOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = {w: i for i, w in enumerate(embeddings_index.keys(), 1)}"
      ],
      "metadata": {
        "id": "zUvEGTH1kRwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "embedding_matrix = np.zeros((len(word_index) + 1,100))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "Qw_ongaygUAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding,GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Conv1D"
      ],
      "metadata": {
        "id": "ml20GEegss8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = Embedding(input_dim=len(word_index) + 1,\n",
        "                            output_dim=100,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=1218,\n",
        "                            trainable=False)"
      ],
      "metadata": {
        "id": "11LwZKwjr_IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    embedding_layer,\n",
        "  Conv1D(256, 10, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "  Dense(500, activation='relu'),\n",
        "  Dense(100, activation='relu'),\n",
        "  Dense(10, activation='relu'),\n",
        "  Dense(4, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "A4kO6brSsGHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow import reduce_mean,reduce_sum\n",
        "# loss1= reduce_mean(reduce_sum(cross_entropy, axis=1))\n",
        "model.compile(loss= \"ce\",optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Yw4gBYopsKGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('drive/MyDrive/Licenta/reponses.json') as json_file:\n",
        "    responses = json.load(json_file)\n",
        "  \n",
        "y_train = responses[\"Sadness\"]\n",
        "import numpy as np\n",
        "# one hot target \n",
        "# momentan avem 4 raspunsuri de la 0 la 3 facem cu I4 la celelalte mai speciale facem cu I8\n",
        "print(y_train)\n",
        "trg = [[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,1.0,0.0],[0.0,0.0,0.0,1.0]]\n",
        "y_train_target = []\n",
        "for j in y_train:\n",
        "  y_train_target.append(trg[j])\n",
        "# y_1 = np.array(y_train_target)\n",
        "# print(type(y_train_target[0][0]))\n",
        "\n",
        "y_test = y_train_target[60:]\n",
        "y_train = y_train_target[0:60]\n"
      ],
      "metadata": {
        "id": "7AA0lZScuuQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea61d24e-0334-4293-a00f-2483f32bdce6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 2, 0, 1, 0, 1, 2, 1, 0, 1, 1, 1, 0, 0, 2, 1, 2, 1, 1, 0, 1, 0, 3, 1, 1, 1, 0, 3, 2, 1, 1, 3, 1, 1, 1, 0, 1, 1, 3, 1, 3, 2, 1, 1, 1, 1, 1, 0, 1, 2, 1, 2, 2, 1, 1, 3, 1, 0, 1, 1, 2, 2, 0, 1, 0, 0, 3, 1, 1, 0, 1, 1, 1, 3, 2, 3, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.asarray(y_test)\n",
        "y_train = np.asarray(y_train)"
      ],
      "metadata": {
        "id": "EiNiM97hwpCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train_padded, y_train, epochs=20, validation_data=(x_test_padded, y_test))"
      ],
      "metadata": {
        "id": "PAtCNv4_sSGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(x_test_padded,y_test)\n",
        "print('Testing Accuracy is {} '.format(accuracy*100))"
      ],
      "metadata": {
        "id": "tJ9U73nDsdMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINAL CNN**"
      ],
      "metadata": {
        "id": "_O2E-rk3sGUT"
      }
    }
  ]
}