{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1wly9gG6PWFlzoznEwGaNGW7MIO2qZDjD",
      "authorship_tag": "ABX9TyMRl5rlyxdQHpZqY8Wf2kCG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madalina0303/Licenta-Depresie/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qA8eHtj4SOLF"
      },
      "outputs": [],
      "source": [
        "from xml.dom.minidom import parse\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
        "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
        "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
        "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
        "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
        "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
        "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
        "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
        "                     \"i'd\": \"i would\", \"i'd've\": \"i would have\",\"i'll\": \"i will\",\n",
        "                     \"i'll've\": \"i will have\",\"i'm\": \"i am\",\"I've\": \"i have\", \"isn't\": \"is not\",\n",
        "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
        "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
        "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
        "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
        "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
        "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
        "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
        "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
        "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
        "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
        "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
        "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
        "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
        "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
        "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
        "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
        "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
        "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
        "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
        "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
        "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
        "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
        "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
        "                     \"you've\": \"you have\"}"
      ],
      "metadata": {
        "id": "jTLM2LZ3Xqdr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)"
      ],
      "metadata": {
        "id": "YaYAtrNjX6I8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADrMdCHReaIr",
        "outputId": "904afb73-dc38-4f28-b2e3-62aeaf0aefa0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.add('subject')\n",
        "stop_words.add('http')\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n"
      ],
      "metadata": {
        "id": "3wemFxPObfTW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])"
      ],
      "metadata": {
        "id": "QcUkbI5wdNCV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_file(filename):\n",
        "    doc = parse(filename, )\n",
        "    titles = doc.getElementsByTagName(\"TITLE\")\n",
        "    posts = doc.getElementsByTagName(\"TEXT\")\n",
        "    messages = []\n",
        "    for index, title in enumerate(titles):\n",
        "        message = title.firstChild.nodeValue + posts[index].firstChild.nodeValue\n",
        "        message = expand_contractions(message).lower()\n",
        "        message= re.sub('[%s]' % re.escape(string.punctuation), '' , message)\n",
        "        message = re.sub('W*dw*','',message)\n",
        "        message = remove_stopwords(message)\n",
        "        message = re.sub('(http[s]?S+)|(w+.[A-Za-z]{2,4}S*)', '', message)\n",
        "        message = lemmatize_words(message)\n",
        "        message  = re.sub(' +', ' ', message)\n",
        "        messages.append(message.strip())\n",
        "        \n",
        "    return messages\n",
        "\n",
        "def get_data_post(dir_name):\n",
        "    users_posts = dict()\n",
        "    for root, dir, files in os.walk(dir_name):\n",
        "        for filename in files:\n",
        "            if \".xml\" in filename:\n",
        "                nr = re.findall(\"[0-9]+\", filename)\n",
        "                users_posts[nr[0]] = parse_file(root+\"/\"+filename)\n",
        "\n",
        "    return users_posts"
      ],
      "metadata": {
        "id": "TFgtdgXdSxot"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " posts = get_data_post(\"drive/MyDrive/data\")\n",
        "#  print(posts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "SJm15b-DS5XK",
        "outputId": "504d6eaa-ac56-43a6-d4aa-96e8b90760ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f9ccda8113a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mposts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/MyDrive/data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#  print(posts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-5024672a306c>\u001b[0m in \u001b[0;36mget_data_post\u001b[0;34m(dir_name)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\".xml\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mnr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[0-9]+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0musers_posts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0musers_posts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-5024672a306c>\u001b[0m in \u001b[0;36mparse_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtitles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetElementsByTagName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TITLE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mposts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetElementsByTagName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TEXT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/xml/dom/minidom.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(file, parser, bufsize)\u001b[0m\n\u001b[1;32m   1956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexpatbuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1958\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mexpatbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpulldom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/xml/dom/expatbuilder.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(file, namespaces)\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/xml/dom/expatbuilder.py\u001b[0m in \u001b[0;36mparseFile\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                 \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfirst_buffer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocumentElement\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m../Modules/pyexpat.c\u001b[0m in \u001b[0;36mStartElement\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/xml/dom/expatbuilder.py\u001b[0m in \u001b[0;36mstart_element_handler\u001b[0;34m(self, name, attributes)\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mlocalname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEMPTY_PREFIX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminidom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mElement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocalname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mownerDocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0m_append_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurNode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/xml/dom/minidom.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tagName, namespaceURI, prefix, localName)\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamespaceURI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamespaceURI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildNodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNodeList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnextSibling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreviousSibling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "r8YexraLqFpQ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dam append sa facem un vector mare din toate postarile, oare cum ar fi cu o matrice ??"
      ],
      "metadata": {
        "id": "ccLhIekjvW-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_post_list(dir_name):\n",
        "    users_posts_list = []\n",
        "    for root, dir, files in os.walk(dir_name):\n",
        "        for filename in files:\n",
        "            if \".xml\" in filename:\n",
        "                nr = re.findall(\"[0-9]+\", filename)\n",
        "                users_posts_list.append(parse_file(root+\"/\"+filename))\n",
        "\n",
        "    return users_posts_list"
      ],
      "metadata": {
        "id": "TnP7YlFvVmC4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = get_data_post_list(\"drive/MyDrive/data\")\n",
        "x_train = all_data[:60]\n",
        "x_test = all_data[60:]\n"
      ],
      "metadata": {
        "id": "gfPDNjMZYi_j"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "vocab_size = 100000\n",
        "oov_token = \"<OOV>\"\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(x_train)"
      ],
      "metadata": {
        "id": "ks_uAhOiage_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "x_test_sequences = tokenizer.texts_to_sequences(x_test)\n"
      ],
      "metadata": {
        "id": "HvEbwiaEauB7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n"
      ],
      "metadata": {
        "id": "XsnPY8QIclYo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_length = 150530\n",
        "padding_type = \"post\"\n",
        "trunction_type=\"post\"\n",
        "x_train_padded = pad_sequences(x_train_sequences, padding=padding_type,\n",
        "                       truncating=trunction_type,maxlen = 1218)\n",
        "x_test_padded = pad_sequences(x_test_sequences,\n",
        "                               padding=padding_type, truncating=trunction_type,maxlen = 1218)"
      ],
      "metadata": {
        "id": "nZUw_ZO4bBAs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(x_train_padded))\n",
        "print(x_train_padded.shape)\n",
        "print(x_test_padded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrzqt63cYPH9",
        "outputId": "88475d1c-fbab-4c46-edca-f968fc9d6e38"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(60, 1218)\n",
            "(20, 1218)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "http://nlp.stanford.edu/data/glove.6B.zip \\\n",
        "-O /tmp/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK0_mdKWemLV",
        "outputId": "160c32bf-1d9e-4624-afa2-ef51114796db"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-07 18:37:24--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-06-07 18:37:24--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-06-07 18:37:25--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘/tmp/glove.6B.zip’\n",
            "\n",
            "/tmp/glove.6B.zip   100%[===================>] 822.24M  5.04MB/s    in 2m 40s  \n",
            "\n",
            "2022-06-07 18:40:05 (5.13 MB/s) - ‘/tmp/glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "with zipfile.ZipFile('/tmp/glove.6B.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/tmp/glove')"
      ],
      "metadata": {
        "id": "PqrLW6vBfdpI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lg = 0\n",
        "import numpy as np\n",
        "embeddings_index = {}\n",
        "f = open('/tmp/glove/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    if len(coefs>lg):\n",
        "      lg = len(coefs)\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print(lg)\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qYl-2dMfpOW",
        "outputId": "2a10b4a4-1ec2-4a25-d1f2-52f25bc245ba"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = {w: i for i, w in enumerate(embeddings_index.keys(), 1)}"
      ],
      "metadata": {
        "id": "zUvEGTH1kRwg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "embedding_matrix = np.zeros((len(word_index) + 1,100))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "Qw_ongaygUAy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding,GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Conv1D"
      ],
      "metadata": {
        "id": "ml20GEegss8G"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = Embedding(input_dim=len(word_index) + 1,\n",
        "                            output_dim=100,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=1218,\n",
        "                            trainable=False)"
      ],
      "metadata": {
        "id": "11LwZKwjr_IW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    embedding_layer,\n",
        "  Conv1D(256, 10, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "  Dense(500, activation='relu'),\n",
        "  Dense(100, activation='relu'),\n",
        "  Dense(10, activation='relu'),\n",
        "  Dense(4, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "A4kO6brSsGHg"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow import reduce_mean,reduce_sum\n",
        "# loss1= reduce_mean(reduce_sum(cross_entropy, axis=1))\n",
        "model.compile(loss= \"mse\",optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Yw4gBYopsKGJ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('drive/MyDrive/Licenta/reponses.json') as json_file:\n",
        "    responses = json.load(json_file)\n",
        "  \n",
        "y_train = responses[\"Sadness\"]\n",
        "import numpy as np\n",
        "# one hot target \n",
        "# momentan avem 4 raspunsuri de la 0 la 3 facem cu I4 la celelalte mai speciale facem cu I8\n",
        "print(y_train)\n",
        "trg = [[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,1.0,0.0],[0.0,0.0,0.0,1.0]]\n",
        "y_train_target = []\n",
        "for j in y_train:\n",
        "  y_train_target.append(trg[j])\n",
        "# y_1 = np.array(y_train_target)\n",
        "# print(type(y_train_target[0][0]))\n",
        "\n",
        "y_test = y_train_target[60:]\n",
        "y_train = y_train_target[0:60]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AA0lZScuuQN",
        "outputId": "73a38dcf-f389-4dbe-ec2c-c070b5f7b601"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 2, 0, 1, 0, 1, 2, 1, 0, 1, 1, 1, 0, 0, 2, 1, 2, 1, 1, 0, 1, 0, 3, 1, 1, 1, 0, 3, 2, 1, 1, 3, 1, 1, 1, 0, 1, 1, 3, 1, 3, 2, 1, 1, 1, 1, 1, 0, 1, 2, 1, 2, 2, 1, 1, 3, 1, 0, 1, 1, 2, 2, 0, 1, 0, 0, 3, 1, 1, 0, 1, 1, 1, 3, 2, 3, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.asarray(y_test)\n",
        "y_train = np.asarray(y_train)"
      ],
      "metadata": {
        "id": "EiNiM97hwpCF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train_padded, y_train, epochs=20, validation_data=(x_test_padded, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAtCNv4_sSGp",
        "outputId": "445aeedf-bc0f-4977-a706-8662fc09e0a0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2/2 [==============================] - 3s 1s/step - loss: 0.1671 - accuracy: 0.5833 - val_loss: 0.1900 - val_accuracy: 0.4500\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 2s 929ms/step - loss: 0.1590 - accuracy: 0.5833 - val_loss: 0.1832 - val_accuracy: 0.4500\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 2s 931ms/step - loss: 0.1350 - accuracy: 0.5833 - val_loss: 0.1784 - val_accuracy: 0.4500\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 2s 968ms/step - loss: 0.1184 - accuracy: 0.5833 - val_loss: 0.1799 - val_accuracy: 0.4500\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 2s 929ms/step - loss: 0.1002 - accuracy: 0.5833 - val_loss: 0.1839 - val_accuracy: 0.4500\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 2s 896ms/step - loss: 0.0779 - accuracy: 0.6833 - val_loss: 0.1854 - val_accuracy: 0.4500\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 2s 919ms/step - loss: 0.0595 - accuracy: 0.7500 - val_loss: 0.2018 - val_accuracy: 0.4500\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 2s 940ms/step - loss: 0.0503 - accuracy: 0.7500 - val_loss: 0.1860 - val_accuracy: 0.4500\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 2s 939ms/step - loss: 0.0474 - accuracy: 0.7500 - val_loss: 0.1928 - val_accuracy: 0.4500\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.0434 - accuracy: 0.7500 - val_loss: 0.2123 - val_accuracy: 0.4500\n",
            "Epoch 11/20\n",
            "2/2 [==============================] - 2s 949ms/step - loss: 0.0415 - accuracy: 0.7500 - val_loss: 0.2216 - val_accuracy: 0.4500\n",
            "Epoch 12/20\n",
            "2/2 [==============================] - 2s 937ms/step - loss: 0.0418 - accuracy: 0.7500 - val_loss: 0.2161 - val_accuracy: 0.4500\n",
            "Epoch 13/20\n",
            "2/2 [==============================] - 2s 933ms/step - loss: 0.0393 - accuracy: 0.7500 - val_loss: 0.2125 - val_accuracy: 0.4500\n",
            "Epoch 14/20\n",
            "2/2 [==============================] - 2s 925ms/step - loss: 0.0389 - accuracy: 0.7500 - val_loss: 0.2054 - val_accuracy: 0.4500\n",
            "Epoch 15/20\n",
            "2/2 [==============================] - 2s 950ms/step - loss: 0.0386 - accuracy: 0.7500 - val_loss: 0.2005 - val_accuracy: 0.4500\n",
            "Epoch 16/20\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.0382 - accuracy: 0.7500 - val_loss: 0.2026 - val_accuracy: 0.4500\n",
            "Epoch 17/20\n",
            "2/2 [==============================] - 2s 939ms/step - loss: 0.0375 - accuracy: 0.7500 - val_loss: 0.2054 - val_accuracy: 0.4500\n",
            "Epoch 18/20\n",
            "2/2 [==============================] - 2s 939ms/step - loss: 0.0367 - accuracy: 0.7500 - val_loss: 0.2100 - val_accuracy: 0.4500\n",
            "Epoch 19/20\n",
            "2/2 [==============================] - 2s 947ms/step - loss: 0.0360 - accuracy: 0.7500 - val_loss: 0.2135 - val_accuracy: 0.4500\n",
            "Epoch 20/20\n",
            "2/2 [==============================] - 2s 944ms/step - loss: 0.0354 - accuracy: 0.7833 - val_loss: 0.2148 - val_accuracy: 0.4500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(x_test_padded,y_test)\n",
        "print('Testing Accuracy is {} '.format(accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ9U73nDsdMx",
        "outputId": "0a2484fc-c8fd-496b-dcd2-cf2bbc3f8a70"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 244ms/step - loss: 0.2148 - accuracy: 0.4500\n",
            "Testing Accuracy is 44.999998807907104 \n"
          ]
        }
      ]
    }
  ]
}