{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1BRwuZ6ATXvB_2TEv_1S-D9pE0BXvuTp6",
      "authorship_tag": "ABX9TyPHdonFYca5AOJPseWG+nMi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madalina0303/Licenta-Depresie/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "G52_8HbKteRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLzU6-MbsUck"
      },
      "outputs": [],
      "source": [
        "from xml.dom.minidom import parse\n",
        "import os\n",
        "from transformers import BertTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "example_text = 'I will watch Memento tonight'\n",
        "bert_input = tokenizer(example_text,padding='max_length', max_length = 10,\n",
        "                       truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "print(bert_input['input_ids'])\n",
        "print(bert_input['token_type_ids'])\n",
        "print(bert_input['attention_mask'])"
      ],
      "metadata": {
        "id": "cudJLonFtfUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence_transformers"
      ],
      "metadata": {
        "id": "ISWEQ_9ZtcMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from pprint import pprint\n"
      ],
      "metadata": {
        "id": "l3q1p831sbA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('all-mpnet-base-v2')"
      ],
      "metadata": {
        "id": "rdstcfA-scBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vuvqERUuqO37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "with open('drive/MyDrive/chestionar.txt', 'r') as f:\n",
        "  data = f.read()\n",
        "  categories = re.split(\"[0-9]+\\.\",data)\n",
        "\n",
        "categories.pop(0)\n",
        "# print(len(categories))\n",
        "for index,elem in enumerate(categories):\n",
        "  categories[index] = elem.replace(\"\\n\",\" \").strip()\n",
        "  # print(categories[index])\n",
        "\n",
        "categories_embedding = model.encode(categories)\n"
      ],
      "metadata": {
        "id": "0_HlvLnTscV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from xml.dom.minidom import parse\n",
        "import os"
      ],
      "metadata": {
        "id": "If_hgzo9zRG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_file(filename):\n",
        "    doc = parse(filename, )\n",
        "    titles = doc.getElementsByTagName(\"TITLE\")\n",
        "    posts = doc.getElementsByTagName(\"TEXT\")\n",
        "    messages = []\n",
        "    for index, title in enumerate(titles):\n",
        "        message = title.firstChild.nodeValue + posts[index].firstChild.nodeValue\n",
        "        messages.append(message.strip())\n",
        "    return messages\n",
        "\n",
        "def get_data_post(dir_name):\n",
        "    users_posts = dict()\n",
        "    for root, dir, files in os.walk(dir_name):\n",
        "        for filename in files:\n",
        "            if \".xml\" in filename:\n",
        "                nr = re.findall(\"[0-9]+\", filename)\n",
        "                users_posts[nr[0]] = parse_file(root+\"/\"+filename)\n",
        "\n",
        "    return users_posts"
      ],
      "metadata": {
        "id": "bKF6pFPqBj-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " posts = get_data_post(\"drive/MyDrive/data\")\n",
        " print(posts)"
      ],
      "metadata": {
        "id": "r7tp1SKzGr8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts_embedding= [ posts[str(i)] for i in range(1,81)]"
      ],
      "metadata": {
        "id": "r_uHalCee1pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((posts_embedding[0]))"
      ],
      "metadata": {
        "id": "sSg10MqmtoGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def get_categories(file):\n",
        "  with open(file) as f:\n",
        "    data = f.read()\n",
        "    questions = re.findall(\"[0-9]+\\.\\s*[a-zA-Z\\s]+\\s*\\n\", data)\n",
        "  \n",
        "  for i, elem in enumerate(questions):\n",
        "    questions[i] = elem.split(\"\\n\")[0]\n",
        "    questions[i] = re.split(\"[0-9]+[\\.\\s]+\",questions[i])[1]\n",
        "    \n",
        "  return questions\n",
        "\n",
        "print(get_categories(\"drive/MyDrive/chestionar.txt\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "_6RUzvYstkuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84a76088-deac-406e-c696-9b5f2dd663cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sadness', 'Pessimism', 'Past Failure', 'Loss of Pleasure', 'Guilty Feelings', 'Punishment Feelings', 'Suicidal Thoughts or Wishes', 'Crying', 'Agitation', 'Loss of Interest', 'Indecisiveness', 'Worthlessness', 'Loss of Energy', 'Changes in Sleeping Pattern', 'Irritability', 'Changes in Appetite', 'Concentration Difficulty', 'Tiredness or Fatigue', 'Loss of Interest in Sex']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories_list = get_categories(\"drive/MyDrive/chestionar.txt\")\n",
        "data_dict = dict()\n",
        "for i in categories_list:\n",
        "  data_dict[i] = dict()\n",
        "  for j in range(1,81):\n",
        "    data_dict[i][str(j)] = []"
      ],
      "metadata": {
        "id": "Y1HVuPa5Iqm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def dump_to_json(file_name,data_dict_param):\n",
        "  with open(file_name, \"w\") as json_file:\n",
        "    json.dump(data_dict_param, json_file)\n",
        "\n",
        "dump_to_json(\"data.json\", data_dict)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1lYTv6eyFVwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts = dict()\n",
        "for i in range(3):\n",
        "  posts[str(i)] = []\n",
        "  posts[str(i)].append(\"3\");\n",
        "dump_to_json(\"ok.json\",posts)"
      ],
      "metadata": {
        "id": "Ib6rlkGs8jOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "posts = dict()\n",
        "for i,elem in enumerate(posts_embedding):\n",
        "  posts[str(i)] = []\n",
        "  for elem2 in elem:\n",
        "    posts[str(i)].append((model.encode(elem2)).tolist())\n",
        "    \n",
        "dump_to_json(\"embedded.json\",posts)"
      ],
      "metadata": {
        "id": "xW88hex1fN6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(posts['1'][0])\n",
        "# new_dict = dict()\n",
        "# for i in range(80):\n",
        "#   new_dict[str[i]]=posts[str(i)].tolist()\n",
        "  "
      ],
      "metadata": {
        "id": "L1BnKPV7dFkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(categories_list)"
      ],
      "metadata": {
        "id": "QKW-_saJi9XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/MyDrive/embedded.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "    "
      ],
      "metadata": {
        "id": "O-nNYV-NhMi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.keys())"
      ],
      "metadata": {
        "id": "pHw0PPy-UMi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['0'])"
      ],
      "metadata": {
        "id": "ylZ6etMVbaX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm"
      ],
      "metadata": {
        "id": "z2kVFeV-uiqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aux = \"drive/MyDrive/\"\n",
        "for c, nume in enumerate(categories_list):\n",
        "  complet = aux + nume + \".json\"\n",
        "  complet1 = aux + nume + \"_ind.json\"\n",
        "  new_dict = dict()\n",
        "  new_dict_index = dict()\n",
        "  for i in range(80):\n",
        "    k = str(i)\n",
        "    # print(type(k))\n",
        "    new_dict[k] = []\n",
        "    new_dict_index[k] = []\n",
        "    j = 0\n",
        "    for elem in data[k]:  \n",
        "      cosine_similarity = np.dot(elem, categories_embedding[c])/(norm(elem)*norm(categories_embedding[c]))\n",
        "      if cosine_similarity >= 0.2:\n",
        "        new_dict[k].append(elem)\n",
        "        new_dict_index[k].append(j)\n",
        "      j = j + 1\n",
        "  dump_to_json(complet,new_dict)\n",
        "  dump_to_json(complet1,new_dict_index)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wr4lIcdvPpnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mutat intr-un folder separat in drive numit licenta \n",
        "import os \n",
        "categories_list = get_categories(\"drive/MyDrive/chestionar.txt\")\n",
        "aux = \"drive/MyDrive/\"\n",
        "new_folder  = \"drive/MyDrive/Licenta/\"\n",
        "for c, nume in enumerate(categories_list):\n",
        "  complet = aux +\"licenta\"+ nume + \".json\"\n",
        "  complet1 = aux + \"licenta\"+nume + \"_ind.json\"\n",
        "  complet_n = new_folder + nume + \".json\"\n",
        "  complet1_n = new_folder + nume + \"_ind.json\"\n",
        "  os.rename(complet,complet_n)\n",
        "  os.rename(complet1, complet1_n)\n",
        "\n"
      ],
      "metadata": {
        "id": "E-Z9wUfuaetO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## primii 60 useri pentru antrenare\n",
        "## urmatorii 20 pentru testare \n",
        "## 75% vs 25%\n",
        "import itertools \n",
        "import json"
      ],
      "metadata": {
        "id": "eMiz_YdKsqT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "aux = \"drive/MyDrive/\"\n",
        "complet = aux + \"Sadness.json\"\n",
        "x_train = []\n",
        "with open(complet) as json_file:\n",
        "   data = json.load(json_file)\n",
        "   max = 0\n",
        "   for vect in data.values():\n",
        "     lg = 0\n",
        "     new_array = []\n",
        "     for i in vect:\n",
        "       \n",
        "       new_array = new_array + i\n",
        "     if len(new_array) > max:\n",
        "        max = len(new_array)\n",
        "     x_train.append(new_array)\n",
        "\n",
        "for elem in x_train:\n",
        "  if len(elem)<max:\n",
        "    elem = elem + [0 for i in range(max-len(elem))]\n",
        "\n",
        "\n",
        "# for c, nume in enumerate(categories_list):\n",
        "#   complet = aux + nume + \".json\"\n"
      ],
      "metadata": {
        "id": "xp16_td8MuF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parasam  fisierul cu raspunsurile bune ca sa preluam targeturile \n"
      ],
      "metadata": {
        "id": "OY5zlO7rZxiu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}