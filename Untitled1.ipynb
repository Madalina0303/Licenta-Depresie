{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1BRwuZ6ATXvB_2TEv_1S-D9pE0BXvuTp6",
      "authorship_tag": "ABX9TyO4wwoD63iCiXFoBUrB0mu+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madalina0303/Licenta-Depresie/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "G52_8HbKteRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLzU6-MbsUck"
      },
      "outputs": [],
      "source": [
        "from xml.dom.minidom import parse\n",
        "import os\n",
        "from transformers import BertTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "example_text = 'I will watch Memento tonight'\n",
        "bert_input = tokenizer(example_text,padding='max_length', max_length = 10,\n",
        "                       truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "print(bert_input['input_ids'])\n",
        "print(bert_input['token_type_ids'])\n",
        "print(bert_input['attention_mask'])"
      ],
      "metadata": {
        "id": "cudJLonFtfUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence_transformers"
      ],
      "metadata": {
        "id": "ISWEQ_9ZtcMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from pprint import pprint\n"
      ],
      "metadata": {
        "id": "l3q1p831sbA1"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('all-mpnet-base-v2')"
      ],
      "metadata": {
        "id": "rdstcfA-scBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vuvqERUuqO37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "with open('drive/MyDrive/chestionar.txt', 'r') as f:\n",
        "  data = f.read()\n",
        "  categories = re.split(\"[0-9]+\\.\",data)\n",
        "\n",
        "categories.pop(0)\n",
        "# print(len(categories))\n",
        "for index,elem in enumerate(categories):\n",
        "  categories[index] = elem.replace(\"\\n\",\" \").strip()\n",
        "  # print(categories[index])\n",
        "\n",
        "categories_embedding = model.encode(categories)\n"
      ],
      "metadata": {
        "id": "0_HlvLnTscV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from xml.dom.minidom import parse\n",
        "import os"
      ],
      "metadata": {
        "id": "If_hgzo9zRG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_file(filename):\n",
        "    doc = parse(filename, )\n",
        "    titles = doc.getElementsByTagName(\"TITLE\")\n",
        "    posts = doc.getElementsByTagName(\"TEXT\")\n",
        "    messages = []\n",
        "    for index, title in enumerate(titles):\n",
        "        message = title.firstChild.nodeValue + posts[index].firstChild.nodeValue\n",
        "        messages.append(message.strip())\n",
        "    return messages\n",
        "\n",
        "def get_data_post(dir_name):\n",
        "    users_posts = dict()\n",
        "    for root, dir, files in os.walk(dir_name):\n",
        "        for filename in files:\n",
        "            if \".xml\" in filename:\n",
        "                nr = re.findall(\"[0-9]+\", filename)\n",
        "                users_posts[nr[0]] = parse_file(root+\"/\"+filename)\n",
        "\n",
        "    return users_posts"
      ],
      "metadata": {
        "id": "bKF6pFPqBj-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " posts = get_data_post(\"drive/MyDrive/data\")\n",
        " print(posts)"
      ],
      "metadata": {
        "id": "r7tp1SKzGr8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts_embedding= [ posts[str(i)] for i in range(1,81)]"
      ],
      "metadata": {
        "id": "r_uHalCee1pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((posts_embedding[0]))"
      ],
      "metadata": {
        "id": "sSg10MqmtoGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def get_categories(file):\n",
        "  with open(file) as f:\n",
        "    data = f.read()\n",
        "    questions = re.findall(\"[0-9]+\\.\\s*[a-zA-Z\\s]+\\s*\\n\", data)\n",
        "  \n",
        "  for i, elem in enumerate(questions):\n",
        "    questions[i] = elem.split(\"\\n\")[0]\n",
        "    questions[i] = re.split(\"[0-9]+[\\.\\s]+\",questions[i])[1]\n",
        "    \n",
        "  return questions\n",
        "\n",
        "print(get_categories(\"drive/MyDrive/chestionar.txt\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "_6RUzvYstkuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84a76088-deac-406e-c696-9b5f2dd663cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sadness', 'Pessimism', 'Past Failure', 'Loss of Pleasure', 'Guilty Feelings', 'Punishment Feelings', 'Suicidal Thoughts or Wishes', 'Crying', 'Agitation', 'Loss of Interest', 'Indecisiveness', 'Worthlessness', 'Loss of Energy', 'Changes in Sleeping Pattern', 'Irritability', 'Changes in Appetite', 'Concentration Difficulty', 'Tiredness or Fatigue', 'Loss of Interest in Sex']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories_list = get_categories(\"drive/MyDrive/chestionar.txt\")\n",
        "data_dict = dict()\n",
        "for i in categories_list:\n",
        "  data_dict[i] = dict()\n",
        "  for j in range(1,81):\n",
        "    data_dict[i][str(j)] = []"
      ],
      "metadata": {
        "id": "Y1HVuPa5Iqm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def dump_to_json(file_name,data_dict_param):\n",
        "  with open(file_name, \"w\") as json_file:\n",
        "    json.dump(data_dict_param, json_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "1lYTv6eyFVwg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dump_to_json(\"data.json\", data_dict)"
      ],
      "metadata": {
        "id": "k7nKoM82rHAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts = dict()\n",
        "for i in range(3):\n",
        "  posts[str(i)] = []\n",
        "  posts[str(i)].append(\"3\");\n",
        "dump_to_json(\"ok.json\",posts)"
      ],
      "metadata": {
        "id": "Ib6rlkGs8jOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "posts = dict()\n",
        "for i,elem in enumerate(posts_embedding):\n",
        "  posts[str(i)] = []\n",
        "  for elem2 in elem:\n",
        "    posts[str(i)].append((model.encode(elem2)).tolist())\n",
        "    \n",
        "dump_to_json(\"embedded.json\",posts)"
      ],
      "metadata": {
        "id": "xW88hex1fN6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(posts['1'][0])\n",
        "# new_dict = dict()\n",
        "# for i in range(80):\n",
        "#   new_dict[str[i]]=posts[str(i)].tolist()\n",
        "  "
      ],
      "metadata": {
        "id": "L1BnKPV7dFkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(categories_list)"
      ],
      "metadata": {
        "id": "QKW-_saJi9XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/MyDrive/embedded.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "    "
      ],
      "metadata": {
        "id": "O-nNYV-NhMi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.keys())"
      ],
      "metadata": {
        "id": "pHw0PPy-UMi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['0'])"
      ],
      "metadata": {
        "id": "ylZ6etMVbaX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm"
      ],
      "metadata": {
        "id": "z2kVFeV-uiqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aux = \"drive/MyDrive/\"\n",
        "for c, nume in enumerate(categories_list):\n",
        "  complet = aux + nume + \".json\"\n",
        "  complet1 = aux + nume + \"_ind.json\"\n",
        "  new_dict = dict()\n",
        "  new_dict_index = dict()\n",
        "  for i in range(80):\n",
        "    k = str(i)\n",
        "    # print(type(k))\n",
        "    new_dict[k] = []\n",
        "    new_dict_index[k] = []\n",
        "    j = 0\n",
        "    for elem in data[k]:  \n",
        "      cosine_similarity = np.dot(elem, categories_embedding[c])/(norm(elem)*norm(categories_embedding[c]))\n",
        "      if cosine_similarity >= 0.2:\n",
        "        new_dict[k].append(elem)\n",
        "        new_dict_index[k].append(j)\n",
        "      j = j + 1\n",
        "  dump_to_json(complet,new_dict)\n",
        "  dump_to_json(complet1,new_dict_index)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wr4lIcdvPpnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mutat intr-un folder separat in drive numit licenta \n",
        "import os \n",
        "categories_list = get_categories(\"drive/MyDrive/chestionar.txt\")\n",
        "aux = \"drive/MyDrive/\"\n",
        "new_folder  = \"drive/MyDrive/Licenta/\"\n",
        "for c, nume in enumerate(categories_list):\n",
        "  complet = aux +\"licenta\"+ nume + \".json\"\n",
        "  complet1 = aux + \"licenta\"+nume + \"_ind.json\"\n",
        "  complet_n = new_folder + nume + \".json\"\n",
        "  complet1_n = new_folder + nume + \"_ind.json\"\n",
        "  os.rename(complet,complet_n)\n",
        "  os.rename(complet1, complet1_n)\n",
        "\n"
      ],
      "metadata": {
        "id": "E-Z9wUfuaetO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## primii 60 useri pentru antrenare\n",
        "## urmatorii 20 pentru testare \n",
        "## 75% vs 25%\n",
        "import itertools \n",
        "import json"
      ],
      "metadata": {
        "id": "eMiz_YdKsqT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "aux = \"drive/MyDrive/Licenta/\"\n",
        "complet = aux + \"Sadness.json\"\n",
        "x_train = []\n",
        "with open(complet) as json_file:\n",
        "   data = json.load(json_file)\n",
        "   max = 0\n",
        "   for vect in data.values():\n",
        "     lg = 0\n",
        "     new_array = []\n",
        "     for i in vect:\n",
        "      #  print(len(i))\n",
        "       new_array = new_array + i\n",
        "     if len(new_array) > max:\n",
        "        max = len(new_array)\n",
        "     x_train.append(new_array)\n",
        "    #  print(len(new_array))\n",
        "\n",
        "# print(\"Maximul este \", max)\n",
        "for ind,elem in enumerate(x_train):\n",
        "  if len(elem)<max:\n",
        "    x_train[ind] = elem + [0]*(max-len(elem))\n",
        "    # print(len(elem))\n",
        "\n",
        "\n",
        "# for c, nume in enumerate(categories_list):\n",
        "#   complet = aux + nume + \".json\"\n"
      ],
      "metadata": {
        "id": "xp16_td8MuF7"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parsam  fisierul cu raspunsurile bune ca sa preluam targeturile \n",
        "response  = \"drive/MyDrive/Licenta/response.txt\"\n",
        "transform_resp = {\"1a\":1, \"1b\":2, \"2a\":3, \"2b\":4, \"3a\":5, \"3b\":6}\n",
        "responses = dict()\n",
        "for j in categories_list:\n",
        "  responses[j]=[]\n",
        "\n",
        "\n",
        "with open(response) as rsp:\n",
        "   data = rsp.readlines()\n",
        "   for line in data:\n",
        "     res = line.split(\" \")[1:]\n",
        "     for ind,j in enumerate(categories_list):\n",
        "      if len(res[ind])>=2:\n",
        "         responses[j].append(transform_resp[res[ind]])\n",
        "      else:\n",
        "       responses[j].append(int(res[ind]))\n",
        "      \n"
      ],
      "metadata": {
        "id": "OY5zlO7rZxiu"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dump_to_json(\"drive/MyDrive/Licenta/reponses.json\",responses)"
      ],
      "metadata": {
        "id": "R2VaiNPkrO_D"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = responses[\"Sadness\"]\n"
      ],
      "metadata": {
        "id": "FEormZ1Jr1l4"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# one hot target \n",
        "# momentan avem 4 raspunsuri de la 0 la 3 facem cu I4 la celelalte mai speciale facem cu I8\n",
        "\n",
        "trg = np.eye(4)\n",
        "y_train_target = []\n",
        "for j in y_train:\n",
        "  y_train_target.append(trg[j])\n",
        "y_1 = np.array(y_train_target)\n",
        "print(y_1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxtWtPI8214L",
        "outputId": "2ad0452c-e0ef-4c43-f16b-13cf2e5127fe"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(80, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train si y_train splituiti ca sa fie doar 60\n",
        "x_test = x_train[60:]\n",
        "x_train = x_train[0:60]\n",
        "y_test = y_1[60:]\n",
        "y_train = y_1[0:60]\n",
        "\n"
      ],
      "metadata": {
        "id": "h9Pd6nyLsap5"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train))\n",
        "# for i in x_train:\n",
        "#   print(len(i))\n",
        "print(type(x_train[0][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvy9LdBAwFdC",
        "outputId": "c8e225f5-56ce-40c9-9a31-3f05ac5fe06c"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60\n",
            "<class 'float'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "print(len(x_train))\n",
        "tensor_x =  torch.as_tensor(x_train) # transform to torch tensor\n",
        "tensor_y =  torch.as_tensor(y_train)\n",
        "\n",
        "my_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "my_dataloader = DataLoader(my_dataset) # create your dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_SngDK-yINM",
        "outputId": "e8d4c405-cbd3-4500-d32d-21e0e5c102de"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(tensor_x[0]))\n",
        "print(max)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk9EFHYabqu1",
        "outputId": "f990b06a-ea66-4404-ef1e-5a2388461f5a"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "150528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import losses\n",
        "\n",
        "loss = losses.SoftmaxLoss(\n",
        "    model=model,\n",
        "    sentence_embedding_dimension=max,\n",
        "    num_labels= 4)  # 0,1,2,3 labels"
      ],
      "metadata": {
        "id": "qIHNRqmDQbj-"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(my_dataloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZfTzAb4X9wO",
        "outputId": "8e68ee26-6dea-4665-d968-42ddb61ef605"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "warmup_steps = int(len(my_dataloader) * epochs * 0.1)  ### NU MERGE NU STIU DE CE , I M SO SAD\n",
        "model.fit(train_objectives=[(my_dataloader,loss)]) ## da eroare tuplu nu are membrul text , dar eu nu am tuple , idkkk"
      ],
      "metadata": {
        "id": "7u9HrRjtUkMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# trecem pe bert \n",
        "# defining 2 input layers for input_ids and attn_masks\n",
        "from transformers import TFBertModel\n",
        "import tensorflow as tf\n",
        "\n",
        "model = TFBertModel.from_pretrained('bert-base-cased') # bert base model with pretrained weights\n",
        "\n",
        "input_ids = tf.keras.layers.Input(shape=(max,), name='input_ids')\n",
        "intermediate_layer = tf.keras.layers.Dense(max, activation='relu', name='intermediate_layer')(x_train)\n",
        "output_layer = tf.keras.layers.Dense(4, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n",
        "\n",
        "sentiment_model = tf.keras.Model(inputs=[x_train], outputs=output_layer)\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\n",
        "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
        "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
        "sentiment_model.compile(optimizer=optim, loss=loss_func, metrics=[acc])\n",
        "hist = sentiment_model.fit(\n",
        "    x_train,\n",
        "    validation_data=y_train,\n",
        "    epochs=2\n",
        ")\n",
        "sentiment_model.save('sentiment_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "fmyHsA5GqhAm",
        "outputId": "62aeeef3-7289-429a-a019-61d2c3a098c8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-aef7d87a9cc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-cased'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# bert base model with pretrained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mintermediate_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'intermediate_layer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0moutput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output_layer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_layer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# softmax -> calcs probs of classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Dimension value must be integer or None or have an __index__ method, got value '<built-in function max>' with type '<class 'builtin_function_or_method'>'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YiHshc-WtQzB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}