{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1BRwuZ6ATXvB_2TEv_1S-D9pE0BXvuTp6",
      "authorship_tag": "ABX9TyNgY/rN4mRDfUk1RgQwUKK6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madalina0303/Licenta-Depresie/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "G52_8HbKteRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLzU6-MbsUck"
      },
      "outputs": [],
      "source": [
        "from xml.dom.minidom import parse\n",
        "import os\n",
        "from transformers import BertTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "example_text = 'I will watch Memento tonight'\n",
        "bert_input = tokenizer(example_text,padding='max_length', max_length = 10,\n",
        "                       truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "print(bert_input['input_ids'])\n",
        "print(bert_input['token_type_ids'])\n",
        "print(bert_input['attention_mask'])"
      ],
      "metadata": {
        "id": "cudJLonFtfUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence_transformers"
      ],
      "metadata": {
        "id": "ISWEQ_9ZtcMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from pprint import pprint\n"
      ],
      "metadata": {
        "id": "l3q1p831sbA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('all-mpnet-base-v2')"
      ],
      "metadata": {
        "id": "rdstcfA-scBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vuvqERUuqO37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "with open('drive/MyDrive/chestionar.txt', 'r') as f:\n",
        "  data = f.read()\n",
        "  categories = re.split(\"[0-9]+\\.\",data)\n",
        "\n",
        "categories.pop(0)\n",
        "# print(len(categories))\n",
        "for index,elem in enumerate(categories):\n",
        "  categories[index] = elem.replace(\"\\n\",\" \").strip()\n",
        "  # print(categories[index])\n",
        "\n",
        "categories_embedding = model.encode(categories)\n"
      ],
      "metadata": {
        "id": "0_HlvLnTscV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from xml.dom.minidom import parse\n",
        "import os"
      ],
      "metadata": {
        "id": "If_hgzo9zRG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_file(filename):\n",
        "    doc = parse(filename, )\n",
        "    titles = doc.getElementsByTagName(\"TITLE\")\n",
        "    posts = doc.getElementsByTagName(\"TEXT\")\n",
        "    messages = []\n",
        "    for index, title in enumerate(titles):\n",
        "        message = title.firstChild.nodeValue + posts[index].firstChild.nodeValue\n",
        "        messages.append(message.strip())\n",
        "    return messages\n",
        "\n",
        "def get_data_post(dir_name):\n",
        "    users_posts = dict()\n",
        "    for root, dir, files in os.walk(dir_name):\n",
        "        for filename in files:\n",
        "            if \".xml\" in filename:\n",
        "                nr = re.findall(\"[0-9]+\", filename)\n",
        "                users_posts[nr[0]] = parse_file(root+\"/\"+filename)\n",
        "\n",
        "    return users_posts"
      ],
      "metadata": {
        "id": "bKF6pFPqBj-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " posts = get_data_post(\"drive/MyDrive/data\")\n",
        " print(posts)"
      ],
      "metadata": {
        "id": "r7tp1SKzGr8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts_embedding= [ posts[str(i)] for i in range(1,81)]"
      ],
      "metadata": {
        "id": "r_uHalCee1pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((posts_embedding[0]))"
      ],
      "metadata": {
        "id": "sSg10MqmtoGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def get_categories(file):\n",
        "  with open(file) as f:\n",
        "    data = f.read()\n",
        "    questions = re.findall(\"[0-9]+\\.\\s*[a-zA-Z\\s-]+\\s*\\n\", data)\n",
        "  \n",
        "  for i, elem in enumerate(questions):\n",
        "    questions[i] = elem.split(\"\\n\")[0]\n",
        "    questions[i] = re.split(\"[0-9]+[\\.\\s]+\",questions[i])[1]\n",
        "    \n",
        "  return questions\n",
        "\n",
        "print(get_categories(\"drive/MyDrive/chestionar.txt\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "_6RUzvYstkuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f29511d3-7d8a-4953-88a3-5bc1f8101eb7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sadness', 'Pessimism', 'Past Failure', 'Loss of Pleasure', 'Guilty Feelings', 'Punishment Feelings', 'Self-Dislike', 'Self-Criticalness', 'Suicidal Thoughts or Wishes', 'Crying', 'Agitation', 'Loss of Interest', 'Indecisiveness', 'Worthlessness', 'Loss of Energy', 'Changes in Sleeping Pattern', 'Irritability', 'Changes in Appetite', 'Concentration Difficulty', 'Tiredness or Fatigue', 'Loss of Interest in Sex']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "utzDjLq3zBZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories_list = get_categories(\"drive/MyDrive/chestionar.txt\")\n",
        "data_dict = dict()\n",
        "for i in categories_list:\n",
        "  data_dict[i] = dict()\n",
        "  for j in range(1,81):\n",
        "    data_dict[i][str(j)] = []"
      ],
      "metadata": {
        "id": "Y1HVuPa5Iqm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def dump_to_json(file_name,data_dict_param):\n",
        "  with open(file_name, \"w\") as json_file:\n",
        "    json.dump(data_dict_param, json_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "1lYTv6eyFVwg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dump_to_json(\"data.json\", data_dict)"
      ],
      "metadata": {
        "id": "k7nKoM82rHAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts = dict()\n",
        "for i in range(3):\n",
        "  posts[str(i)] = []\n",
        "  posts[str(i)].append(\"3\");\n",
        "dump_to_json(\"ok.json\",posts)"
      ],
      "metadata": {
        "id": "Ib6rlkGs8jOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "posts = dict()\n",
        "for i,elem in enumerate(posts_embedding):\n",
        "  posts[str(i)] = []\n",
        "  for elem2 in elem:\n",
        "    posts[str(i)].append((model.encode(elem2)).tolist())\n",
        "    \n",
        "dump_to_json(\"embedded.json\",posts)"
      ],
      "metadata": {
        "id": "xW88hex1fN6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(posts['1'][0])\n",
        "# new_dict = dict()\n",
        "# for i in range(80):\n",
        "#   new_dict[str[i]]=posts[str(i)].tolist()\n",
        "  "
      ],
      "metadata": {
        "id": "L1BnKPV7dFkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(categories_list))"
      ],
      "metadata": {
        "id": "QKW-_saJi9XR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d2a13f-d7c0-4b0a-a84a-b7d072dd38f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/MyDrive/embedded.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "    "
      ],
      "metadata": {
        "id": "O-nNYV-NhMi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.keys())"
      ],
      "metadata": {
        "id": "pHw0PPy-UMi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['0'])"
      ],
      "metadata": {
        "id": "ylZ6etMVbaX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm"
      ],
      "metadata": {
        "id": "z2kVFeV-uiqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aux = \"drive/MyDrive/\"\n",
        "for c, nume in enumerate(categories_list):\n",
        "  complet = aux + nume + \".json\"\n",
        "  complet1 = aux + nume + \"_ind.json\"\n",
        "  new_dict = dict()\n",
        "  new_dict_index = dict()\n",
        "  for i in range(80):\n",
        "    k = str(i)\n",
        "    # print(type(k))\n",
        "    new_dict[k] = []\n",
        "    new_dict_index[k] = []\n",
        "    j = 0\n",
        "    for elem in data[k]:  \n",
        "      cosine_similarity = np.dot(elem, categories_embedding[c])/(norm(elem)*norm(categories_embedding[c]))\n",
        "      if cosine_similarity >= 0.2:\n",
        "        new_dict[k].append(elem)\n",
        "        new_dict_index[k].append(j)\n",
        "      j = j + 1\n",
        "  dump_to_json(complet,new_dict)\n",
        "  dump_to_json(complet1,new_dict_index)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wr4lIcdvPpnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mutat intr-un folder separat in drive numit licenta \n",
        "import os \n",
        "categories_list = get_categories(\"drive/MyDrive/chestionar.txt\")\n",
        "aux = \"drive/MyDrive/\"\n",
        "new_folder  = \"drive/MyDrive/Licenta/\"\n",
        "for c, nume in enumerate(categories_list):\n",
        "  complet = aux +\"licenta\"+ nume + \".json\"\n",
        "  complet1 = aux + \"licenta\"+nume + \"_ind.json\"\n",
        "  complet_n = new_folder + nume + \".json\"\n",
        "  complet1_n = new_folder + nume + \"_ind.json\"\n",
        "  os.rename(complet,complet_n)\n",
        "  os.rename(complet1, complet1_n)\n",
        "\n"
      ],
      "metadata": {
        "id": "E-Z9wUfuaetO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## primii 60 useri pentru antrenare\n",
        "## urmatorii 20 pentru testare \n",
        "## 75% vs 25%\n",
        "import itertools \n",
        "import json"
      ],
      "metadata": {
        "id": "eMiz_YdKsqT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "aux = \"drive/MyDrive/Licenta/\"\n",
        "complet = aux + \"Sadness.json\"\n",
        "x_train = []\n",
        "with open(complet) as json_file:\n",
        "   data = json.load(json_file)\n",
        "   max1 = 0\n",
        "   for vect in data.values():\n",
        "     lg = 0\n",
        "     new_array = []\n",
        "     for i in vect:\n",
        "      #  print(len(i))\n",
        "       new_array = new_array + i\n",
        "     if len(new_array) > max1:\n",
        "        max1 = len(new_array)\n",
        "     x_train.append(new_array)\n",
        "    #  print(len(new_array))\n",
        "\n",
        "# print(\"Maximul este \", max1)\n",
        "for ind,elem in enumerate(x_train):\n",
        "  if len(elem)<max1:\n",
        "    x_train[ind] = elem + [0]*(max1-len(elem))\n",
        "    # print(len(elem))\n",
        "\n",
        "\n",
        "# for c, nume in enumerate(categories_list):\n",
        "#   complet = aux + nume + \".json\"\n"
      ],
      "metadata": {
        "id": "xp16_td8MuF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parsam  fisierul cu raspunsurile bune ca sa preluam targeturile \n",
        "response  = \"drive/MyDrive/Licenta/response.txt\"\n",
        "transform_resp = {\"1a\":1, \"1b\":2, \"2a\":3, \"2b\":4, \"3a\":5, \"3b\":6}\n",
        "responses = dict()\n",
        "for j in categories_list:\n",
        "  responses[j]=[]\n",
        "\n",
        "\n",
        "with open(response) as rsp:\n",
        "   data = rsp.readlines()\n",
        "   for index,line in enumerate(data):\n",
        "     res = line[:-1].split(\" \")[1:]\n",
        "    #  print(len(res))\n",
        "    #  if len(res)==20:\n",
        "    #     print(index)\n",
        "    #     print(res)\n",
        "     for ind,j in enumerate(categories_list):\n",
        "      if len(res[ind])>=2:\n",
        "         responses[j].append(transform_resp[res[ind]])\n",
        "      else:\n",
        "       responses[j].append(int(res[ind]))\n",
        "      \n"
      ],
      "metadata": {
        "id": "OY5zlO7rZxiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dump_to_json(\"drive/MyDrive/Licenta/reponses.json\",responses)"
      ],
      "metadata": {
        "id": "R2VaiNPkrO_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('drive/MyDrive/Licenta/reponses.json') as json_file:\n",
        "    responses = json.load(json_file)"
      ],
      "metadata": {
        "id": "08vkE-xQbvEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = responses[\"Sadness\"]\n"
      ],
      "metadata": {
        "id": "FEormZ1Jr1l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# one hot target \n",
        "# momentan avem 4 raspunsuri de la 0 la 3 facem cu I4 la celelalte mai speciale facem cu I8\n",
        "print(y_train)\n",
        "trg = [[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,1.0,0.0],[0.0,0.0,0.0,1.0]]\n",
        "y_train_target = []\n",
        "for j in y_train:\n",
        "  y_train_target.append(trg[j])\n",
        "# y_1 = np.array(y_train_target)\n",
        "# print(type(y_train_target[0][0]))"
      ],
      "metadata": {
        "id": "uxtWtPI8214L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train si y_train splituiti ca sa fie doar 60\n",
        "x_test = x_train[60:]\n",
        "x_train = x_train[0:60]\n",
        "\n"
      ],
      "metadata": {
        "id": "h9Pd6nyLsap5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_test = y_train_target[60:]\n",
        "y_train = y_train_target[0:60]"
      ],
      "metadata": {
        "id": "bcXGtQyAwni4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train))\n",
        "# for i in x_train:\n",
        "#   print(len(i))\n",
        "print(type(x_train[0][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvy9LdBAwFdC",
        "outputId": "52fdf8f9-820f-4231-d10c-ba05e4bf9970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60\n",
            "<class 'float'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(y_train[0][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iODiApzXbsjf",
        "outputId": "1371a907-574c-451f-af4c-efe5d1147466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'float'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "print(len(x_train))\n",
        "tensor_x =  torch.as_tensor(x_train) # transform to torch tensor\n",
        "tensor_y =  torch.as_tensor(y_train)\n",
        "\n",
        "my_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "my_dataloader = DataLoader(my_dataset) # create your dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_SngDK-yINM",
        "outputId": "44eff5f8-7364-4ff0-e8ec-cdc98c878aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(tensor_x[0]))\n",
        "print(max1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk9EFHYabqu1",
        "outputId": "645632e2-e886-4ebd-8032-1f2e2cdded57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "150528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import losses\n",
        "\n",
        "loss = losses.SoftmaxLoss(\n",
        "    model=model,\n",
        "    sentence_embedding_dimension=max1,\n",
        "    num_labels= 4)  # 0,1,2,3 labels"
      ],
      "metadata": {
        "id": "qIHNRqmDQbj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(my_dataloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZfTzAb4X9wO",
        "outputId": "bc6b6a8f-e1d6-4d9a-93c0-027a5c0e7786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "warmup_steps = int(len(my_dataloader) * epochs * 0.1)  ### NU MERGE NU STIU DE CE , I M SO SAD\n",
        "model.fit(train_objectives=[(my_dataloader,loss)]) ## da eroare tuplu nu are membrul text , dar eu nu am tuple , idkkk"
      ],
      "metadata": {
        "id": "7u9HrRjtUkMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# trecem pe bert \n",
        "# defining 2 input layers for input_ids and attn_masks\n",
        "# nu a  mers, eroare pe undeva pe acolo idk de ce \n",
        "from transformers import TFBertModel\n",
        "import tensorflow as tf\n",
        "\n",
        "model = TFBertModel.from_pretrained('bert-base-cased') # bert base model with pretrained weights\n",
        "\n",
        "input_ids = tf.keras.layers.Input(shape=(150528,), name='input_ids')\n",
        "intermediate_layer = tf.keras.layers.Dense(500, activation='relu', name='intermediate_layer')(input_ids)\n",
        "output_layer = tf.keras.layers.Dense(4, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n",
        "\n",
        "sentiment_model = tf.keras.Model(inputs=[x_train], outputs=output_layer)\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\n",
        "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
        "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
        "sentiment_model.compile(optimizer=optim, loss=loss_func, metrics=[acc])\n",
        "hist = sentiment_model.fit(\n",
        "    x_train,\n",
        "    validation_data=y_train,\n",
        "    epochs=2 \n",
        ")\n",
        "print(acc)\n",
        "sentiment_model.save('drive/MyDrive/Licenta/sentiment_model')"
      ],
      "metadata": {
        "id": "fmyHsA5GqhAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1000, input_dim=150528, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "# compile the keras model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit the keras model on the dataset\n",
        "model.fit(x_train, y_train, epochs=150, batch_size=10)\n",
        "# evaluate the keras model\n",
        "_, accuracy = model.evaluate(x_train, y_train)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n",
        "model.save('drive/MyDrive/Licenta/modelSecv') # acuratete 95 % cred ca e ok pe datul de antrenare sa vedem pe cel de testare \n"
      ],
      "metadata": {
        "id": "eDulV8qjdZto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print(\"test loss, test acc:\", results) # doar 40 la suta acuratetea\n"
      ],
      "metadata": {
        "id": "YiHshc-WtQzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results2 = model.evaluate(x_test, y_test)\n",
        "print(\"test loss, test acc:\", results2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INqBwiKV6G3p",
        "outputId": "a5b2c8ef-1b9a-4d15-d81a-d52cbf725200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 277ms/step - loss: 4.2877 - accuracy: 0.4000\n",
            "test loss, test acc: [4.287680149078369, 0.4000000059604645]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import adam_v2\n",
        "model = Sequential()\n",
        "model.add(Dense(1000, input_dim=150528, activation='relu'))\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "optim = adam_v2.Adam(learning_rate=1e-5, decay=1e-6)\n",
        "# compile the keras model\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
        "# fit the keras model on the dataset\n",
        "model.fit(x_train, y_train, epochs=150, batch_size=10)\n",
        "# evaluate the keras model\n",
        "_, accuracy = model.evaluate(x_train, y_train)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n",
        "model.save('drive/MyDrive/Licenta/modelSecv2multestraturi')"
      ],
      "metadata": {
        "id": "KdsyRd_1NYVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results2 = model.evaluate(x_test, y_test)\n",
        "print(\"test loss, test acc:\", results2)"
      ],
      "metadata": {
        "id": "n7-ZXIkFeDvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "# model = keras.models.load_model('drive/MyDrive/Licenta/modelSecv2multestraturi')"
      ],
      "metadata": {
        "id": "Jo1u09gaX2O7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import adam_v2\n",
        "import json"
      ],
      "metadata": {
        "id": "D6teHFqDg2cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_network(category, x_train, x_test, y_train, y_test,dimi,nr):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(1000, input_dim=dimi, activation='relu'))\n",
        "  model.add(Dense(500, activation='relu'))\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(nr, activation='softmax'))\n",
        "  optim = adam_v2.Adam(learning_rate=1e-5, decay=1e-6)\n",
        "  # compile the keras model\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
        "  # fit the keras model on the dataset\n",
        "  model.fit(x_train, y_train, epochs=150, batch_size=10)\n",
        "  # evaluate the keras model\n",
        "  _, accuracy = model.evaluate(x_test, y_test)\n",
        "  print('Accuracy: %.2f' % (accuracy*100))\n",
        "  path_to_save = \"drive/MyDrive/ModeleSalvate/\"+ category\n",
        "  model.save(path_to_save)\n"
      ],
      "metadata": {
        "id": "WBVWvhySYBcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_test_data(category):\n",
        "  aux = \"drive/MyDrive/Licenta/\"\n",
        "  complet = aux +category+\".json\"\n",
        "  x_train = []\n",
        "  with open(complet) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    max1 = 0\n",
        "    for vect in data.values():\n",
        "      lg = 0\n",
        "      new_array = []\n",
        "      for i in vect:\n",
        "        #  print(len(i))\n",
        "        new_array = new_array + i\n",
        "      if len(new_array) > max1:\n",
        "          max1 = len(new_array)\n",
        "      x_train.append(new_array)\n",
        "      #  print(len(new_array))\n",
        "\n",
        "  # print(\"Maximul este \", max1)\n",
        "  for ind,elem in enumerate(x_train):\n",
        "    if len(elem)<max1:\n",
        "      x_train[ind] = elem + [0]*(max1-len(elem))\n",
        "\n",
        "\n",
        "  with open('drive/MyDrive/Licenta/reponses.json') as json_file:\n",
        "    responses = json.load(json_file)\n",
        "  y_train = responses[category]\n",
        "  trg = [[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,1.0,0.0],[0.0,0.0,0.0,1.0]]\n",
        "  nr = 4\n",
        "  if category == \"Changes in Sleeping Pattern\" or category == \"Changes in Appetite\":\n",
        "    trg = [[1.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,1.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,1.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,1.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,1.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,1.0]]\n",
        "    nr = 7\n",
        "  y_train_target = []\n",
        "  for j in y_train:\n",
        "    y_train_target.append(trg[j])\n",
        "  \n",
        "  return x_train[0:60], y_train_target[0:60], x_train[60:],  y_train_target[60:],len(x_train[0]),nr"
      ],
      "metadata": {
        "id": "qtBG1N9Fb5v8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def get_categories(file):\n",
        "  with open(file) as f:\n",
        "    data = f.read()\n",
        "    questions = re.findall(\"[0-9]+\\.\\s*[a-zA-Z\\s]+\\s*\\n\", data)\n",
        "  \n",
        "  for i, elem in enumerate(questions):\n",
        "    questions[i] = elem.split(\"\\n\")[0]\n",
        "    questions[i] = re.split(\"[0-9]+[\\.\\s]+\",questions[i])[1]\n",
        "    \n",
        "  return questions\n",
        "\n"
      ],
      "metadata": {
        "id": "ccKcY5fCdsL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_categories = get_categories(\"drive/MyDrive/chestionar.txt\")"
      ],
      "metadata": {
        "id": "eGSQbafte6iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_categories))\n",
        "print(len(all_categories))\n",
        "print(all_categories[15:])"
      ],
      "metadata": {
        "id": "7C3j_H-xykFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in all_categories[15:]:\n",
        "  x_train,y_train,x_test,y_test,dimi,nr = get_train_test_data(c)\n",
        "  # print(len(x_train[1]))\n",
        "  # print(x_train[0:10])\n",
        "  build_network(c,x_train,x_test,y_train,y_test,dimi,nr)  "
      ],
      "metadata": {
        "id": "FL7dFo7Be_l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(category):\n",
        "  path_to_load = \"drive/MyDrive/ModeleSalvate/\"\n",
        "  reconstructed_model = keras.models.load_model(path_to_load+category) \n",
        "  x_train,y_train,x_test,y_test,dimi,nr = get_train_test_data(category)\n",
        "  out = reconstructed_model.predict(x_test)\n",
        "  return out"
      ],
      "metadata": {
        "id": "Ce27Lcscw6_K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28CNEbxr6tnS",
        "outputId": "1b47a6f1-ef48-444c-f612-e5274fba337e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.backend import argmax,cast\n",
        "import tensorflow as tf\n",
        "import json\n",
        "# print (type(out))\n",
        "# for o in out:\n",
        "#   print(type(argmax(o).numpy().item()))"
      ],
      "metadata": {
        "id": "LVOf89EL1jur"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_responses = dict()\n",
        "# for i in range(61,81):\n",
        "#   all_responses[str(i)]= []\n",
        "# print(len(all_responses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeXFdvkQ9ayu",
        "outputId": "cdd0b07e-45cb-44d6-f596-d95007674cc6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_categories = get_categories(\"drive/MyDrive/chestionar.txt\")\n",
        "all_responses = dict()\n",
        "for i in range(61,81):\n",
        "  all_responses[str(i)]= []\n",
        "for c in all_categories:\n",
        "  r = get_model(c)\n",
        "  for index,o in enumerate(r):\n",
        "    nr = argmax(o).numpy().item()\n",
        "    all_responses[str(61+index)].append(nr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iK706XR782s",
        "outputId": "6c725141-f687-4064-e682-fba1612ba2c8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc6a3478440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc6abc7ad40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dump_to_json(\"drive/MyDrive/Licenta/predictedResponses.json\", all_responses)"
      ],
      "metadata": {
        "id": "O9UXAcg49vrF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_correct_responses():\n",
        "  correct = dict()\n",
        "  for i in range(61,81):\n",
        "    correct[str(i)]= []\n",
        "  response  = \"drive/MyDrive/Licenta/response.txt\"\n",
        "  transform_resp = {\"1a\":1, \"1b\":2, \"2a\":3, \"2b\":4, \"3a\":5, \"3b\":6}\n",
        "  with open(response) as rsp:\n",
        "    data = rsp.readlines()[60:]\n",
        "    for index,line in enumerate(data):\n",
        "      res = line[:-1].split(\" \")[1:]\n",
        "      for j in res:\n",
        "        if len(j)<=1:\n",
        "          correct[str(61+index)].append(int(j))\n",
        "        else:\n",
        "            correct[str(61+index)].append(int(transform_resp[j]))\n",
        "    return correct\n",
        "\n",
        "corr = get_correct_responses()\n",
        "print(corr)     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0PwzAj0FBSL",
        "outputId": "029cd649-01e9-47ce-ef87-f025ee4c733d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'61': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0], '62': [1, 0, 1, 2, 1, 1, 1, 2, 0, 0, 1, 1, 2, 1, 0, 2, 0, 1, 1, 1, 0], '63': [1, 2, 3, 1, 1, 0, 3, 2, 1, 3, 2, 2, 3, 2, 2, 3, 2, 2, 1, 1, 1], '64': [2, 3, 2, 3, 1, 2, 3, 2, 1, 0, 3, 3, 3, 3, 3, 2, 1, 3, 2, 3, 2], '65': [2, 2, 3, 2, 3, 2, 2, 3, 1, 3, 2, 2, 3, 3, 3, 3, 2, 0, 1, 2, 0], '66': [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 2, 1, 0, 0, 1, 1, 1, 2, 2, 1, 2], '67': [1, 1, 3, 2, 3, 2, 3, 3, 2, 1, 1, 1, 3, 3, 1, 0, 1, 6, 2, 3, 1], '68': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0], '69': [0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 4, 1, 4, 2, 2, 1], '70': [3, 2, 2, 1, 2, 2, 2, 3, 1, 1, 2, 2, 2, 2, 2, 4, 2, 1, 2, 1, 0], '71': [1, 1, 0, 3, 1, 3, 2, 2, 0, 2, 2, 3, 1, 1, 3, 4, 0, 6, 2, 3, 3], '72': [1, 1, 0, 1, 0, 2, 1, 1, 0, 2, 1, 1, 3, 2, 0, 6, 0, 1, 2, 1, 0], '73': [0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 0, 0], '74': [1, 2, 1, 1, 0, 0, 0, 0, 1, 3, 1, 2, 1, 0, 2, 5, 2, 2, 2, 2, 2], '75': [1, 0, 1, 1, 3, 1, 3, 0, 1, 0, 1, 1, 2, 2, 2, 4, 1, 6, 3, 3, 1], '76': [1, 0, 0, 1, 2, 1, 2, 2, 1, 3, 1, 2, 1, 1, 1, 0, 2, 4, 1, 1, 2], '77': [3, 1, 0, 1, 0, 1, 1, 1, 1, 3, 1, 1, 0, 0, 0, 0, 1, 3, 1, 0, 0], '78': [2, 3, 2, 1, 1, 2, 2, 1, 2, 3, 2, 1, 1, 3, 1, 6, 1, 0, 2, 0, 3], '79': [3, 2, 2, 1, 1, 0, 3, 1, 2, 1, 1, 1, 0, 2, 2, 3, 1, 5, 2, 3, 1], '80': [1, 1, 2, 1, 1, 1, 3, 2, 0, 0, 1, 1, 0, 1, 1, 0, 1, 3, 2, 1, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hr(l1, l2):\n",
        "  c = 0\n",
        "  for index, e in enumerate(l1):\n",
        "    if e == l2[index]:\n",
        "      c += 1\n",
        "  return c/21"
      ],
      "metadata": {
        "id": "H9n96dEFDuLd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ahr():\n",
        "  s = 0\n",
        "  for i in range(61,81):\n",
        "    l1 = all_responses[str(i)]\n",
        "    l2 = corr[str(i)]\n",
        "    s += hr(l1,l2)\n",
        "  return s/20"
      ],
      "metadata": {
        "id": "-n4La2cMI75c"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ahr()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHJT48FwJSvo",
        "outputId": "bfd19134-a119-4594-e5a5-f7666671e902"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3357142857142857"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dict_transform(elem):\n",
        "  tr_dict={\"0\":0,\"1\":1,\"2\":1,\"3\":2,\"4\":2,\"5\":3,\"6\":3}\n",
        "  return tr_dict[str(elem)]\n"
      ],
      "metadata": {
        "id": "eJjbzTWAMpWb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cr(l1,l2):\n",
        "  s = 0\n",
        "  # if mad == 7:\n",
        "  #   new_l1 = transform(l1)\n",
        "  #   new_l2 = transform(l2)\n",
        "  for index, e in enumerate(l1):\n",
        "    if index == 15 or index == 17:\n",
        "      mad = 7\n",
        "      e = dict_transform(e)\n",
        "      l2[index] = dict_transform(l2[index])\n",
        "    else:\n",
        "      mad = 4\n",
        "    ad = abs(e-l2[index]) \n",
        "    cr = (mad-ad)/mad\n",
        "    s = s+cr\n",
        "  return s/21"
      ],
      "metadata": {
        "id": "Y3YgxU6MKuy1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def acr():\n",
        "  s = 0\n",
        "  for i in range(61,81):\n",
        "    l1 = all_responses[str(i)]\n",
        "    l2 = corr[str(i)]\n",
        "    s += cr(l1,l2)\n",
        "  return s/20"
      ],
      "metadata": {
        "id": "JlIHWL-oNmBY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acr()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7usdEmmPQcaw",
        "outputId": "39a38cbe-cb9b-4b4a-9197-c4d9b792e2f0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7776360544217685"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dodl(l1,l2):\n",
        "  s1 = 0\n",
        "  s2 = 0\n",
        "  for index, e in enumerate(l1):\n",
        "    if index == 15 or index == 17:\n",
        "      e = dict_transform(e)\n",
        "      l2[index] = dict_transform(l2[index])\n",
        "    s1 += e\n",
        "    s2 += l2[index]\n",
        "  ad_overall = abs(s1-s2)\n",
        "  return (63-ad_overall)/63\n"
      ],
      "metadata": {
        "id": "fMKsiDHARG5D"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adodl():\n",
        "  s = 0\n",
        "  for i in range(61,81):\n",
        "    l1 = all_responses[str(i)]\n",
        "    l2 = corr[str(i)]\n",
        "    s += dodl(l1,l2)\n",
        "  return s/20"
      ],
      "metadata": {
        "id": "VDJBw7TnR5UU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adodl()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOAKBP3PSCQ5",
        "outputId": "47b387c9-abf8-448c-f51e-2639f5ea7293"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8531746031746034"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dchr(l1,l2):\n",
        "  s1 = 0\n",
        "  s2 = 0\n",
        "  for index, e in enumerate(l1):\n",
        "    if index == 15 or index == 17:\n",
        "      e = dict_transform(e)\n",
        "      l2[index] = dict_transform(l2[index])\n",
        "    s1 += e\n",
        "    s2 += l2[index]\n",
        "  n1 = -1\n",
        "  n2 = -1\n",
        "  if s1>=0 and s1<=9:\n",
        "    n1 = 1\n",
        "  elif s1>=10 and s1<=18:\n",
        "    n1 = 2\n",
        "  elif s1>=19 and s1<=29:\n",
        "    n1 = 3\n",
        "  else:\n",
        "    n1 = 4\n",
        "  \n",
        "  if s2>=0 and s2<=9:\n",
        "    n2 = 1\n",
        "  elif s2>=10 and s2<=18:\n",
        "    n2 = 2\n",
        "  elif s2>=19 and s2<=29:\n",
        "    n2 = 3\n",
        "  else:\n",
        "    n2 = 4\n",
        "  \n",
        "  if n1 == n2:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k4RnZv-ASP2V"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adchr():\n",
        "  s = 0\n",
        "  for i in range(61,81):\n",
        "    l1 = all_responses[str(i)]\n",
        "    l2 = corr[str(i)]\n",
        "    s += dchr(l1,l2)\n",
        "  return s/20"
      ],
      "metadata": {
        "id": "cf5HfQKrT1cx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adchr()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mgzWDnyUBvQ",
        "outputId": "367f6da4-a1ad-42df-9207-670f78c8e29a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.35"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}