{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alti-Clasificatori.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1m57LgOv7N2lZqFBgyGXjbuZOPuuphnJR",
      "authorship_tag": "ABX9TyOkH2F87B+ChVK1FAFcNHM3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madalina0303/Licenta-Depresie/blob/main/Alti_Clasificatori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qA8eHtj4SOLF"
      },
      "outputs": [],
      "source": [
        "from xml.dom.minidom import parse\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
        "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
        "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
        "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
        "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
        "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
        "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
        "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
        "                     \"i'd\": \"i would\", \"i'd've\": \"i would have\",\"i'll\": \"i will\",\n",
        "                     \"i'll've\": \"i will have\",\"i'm\": \"i am\",\"I've\": \"i have\", \"isn't\": \"is not\",\n",
        "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
        "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
        "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
        "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
        "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
        "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
        "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
        "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
        "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
        "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
        "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
        "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
        "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
        "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
        "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
        "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
        "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
        "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
        "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
        "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
        "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
        "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
        "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
        "                     \"you've\": \"you have\"}"
      ],
      "metadata": {
        "id": "jTLM2LZ3Xqdr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)"
      ],
      "metadata": {
        "id": "YaYAtrNjX6I8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADrMdCHReaIr",
        "outputId": "7d076213-d818-436a-d32b-00142c49c446"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.add('subject')\n",
        "stop_words.add('http')\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n"
      ],
      "metadata": {
        "id": "3wemFxPObfTW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])"
      ],
      "metadata": {
        "id": "QcUkbI5wdNCV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_file(filename):\n",
        "    doc = parse(filename, )\n",
        "    titles = doc.getElementsByTagName(\"TITLE\")\n",
        "    posts = doc.getElementsByTagName(\"TEXT\")\n",
        "    messages = []\n",
        "    for index, title in enumerate(titles):\n",
        "        message = title.firstChild.nodeValue + posts[index].firstChild.nodeValue\n",
        "        message = expand_contractions(message).lower()\n",
        "        message= re.sub('[%s]' % re.escape(string.punctuation), '' , message)\n",
        "        message = re.sub('W*dw*','',message)\n",
        "        message = remove_stopwords(message)\n",
        "        message = re.sub('(http[s]?S+)|(w+.[A-Za-z]{2,4}S*)', '', message)\n",
        "        message = lemmatize_words(message)\n",
        "        message  = re.sub(' +', ' ', message)\n",
        "        messages.append(message.strip())\n",
        "        \n",
        "    return messages\n"
      ],
      "metadata": {
        "id": "TFgtdgXdSxot"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "r8YexraLqFpQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dam append sa facem un vector mare din toate postarile, oare cum ar fi cu o matrice ??"
      ],
      "metadata": {
        "id": "ccLhIekjvW-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_post_list(dir_name):\n",
        "    users_posts_list = []\n",
        "    for root, dir, files in os.walk(dir_name):\n",
        "        for filename in files:\n",
        "            if \".xml\" in filename:\n",
        "                nr = re.findall(\"[0-9]+\", filename)\n",
        "                users_posts_list.append(parse_file(root+\"/\"+filename))\n",
        "\n",
        "    return users_posts_list"
      ],
      "metadata": {
        "id": "TnP7YlFvVmC4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = get_data_post_list(\"drive/MyDrive/data\")\n",
        "x_train = all_data[:60]\n",
        "x_test = all_data[60:]\n"
      ],
      "metadata": {
        "id": "gfPDNjMZYi_j"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = 1000)\n",
        " # Stop words are common words in English that don't tell us anything about the polarity of a review.\n",
        "    # Such words include the, that, and a\n",
        "# Converts a collection of text documents to a matrix of token counts\n",
        "# max_features = maximum number of words we'd like to have in our bag of words model\n",
        "X = []\n",
        "for i in all_data:\n",
        "  X.append(cv.fit_transform(i).toarray())"
      ],
      "metadata": {
        "id": "gm0IyL2vsdGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.get_feature_names()"
      ],
      "metadata": {
        "id": "350VGLQLt2zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tf_transformer = TfidfTransformer()\n",
        "X = tf_transformer.fit_transform(X).toarray()"
      ],
      "metadata": {
        "id": "7QKRGkyYu3qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidfVectorizer = TfidfVectorizer(max_features =1000)\n",
        "X = []\n",
        "for i in all_data:\n",
        "  X.append(tfidfVectorizer.fit_transform(i).toarray())\n",
        "  \n"
      ],
      "metadata": {
        "id": "Z4VFRMLi4jS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 5000) \n",
        "for i in all_data:\n",
        "  vectorizer.fit(i) \n",
        "  X.append(tfidfVectorizer.transform(i).toarray())\n",
        "\n",
        "# Xtrain = vectorizer.transform(all_data)\n",
        "print(len(X[0]))\n",
        "print(len(X[1]))"
      ],
      "metadata": {
        "id": "TGe5bt2k-BYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(X[1]))\n",
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuQFu8-v5IKJ",
        "outputId": "9b9aa1e2-6ae2-42ad-d8b9-6c0b8c0f4b95"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.0.6-cp37-none-manylinux1_x86_64.whl (76.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 76.6 MB 76 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (4.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (8.0.1)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[1])"
      ],
      "metadata": {
        "id": "IPd9acJF9Q9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Naive Bayes is a statistical classification technique based on Bayes Theorem\n",
        "# common classifier used in sentiment analysis is the Naive Bayes Classifier.\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier # this is experimental\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "classifiers = [GradientBoostingClassifier(),GaussianNB(),HistGradientBoostingClassifier(),\n",
        "               RandomForestClassifier(),LogisticRegression(),XGBClassifier(),LGBMClassifier(),\n",
        "               CatBoostClassifier(verbose=0)]"
      ],
      "metadata": {
        "id": "-2eZarX-5YGF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('drive/MyDrive/Licenta/reponses.json') as json_file:\n",
        "    responses = json.load(json_file)\n",
        "  \n",
        "y_train = responses[\"Sadness\"]\n",
        "import numpy as np\n",
        "# one hot target \n",
        "# momentan avem 4 raspunsuri de la 0 la 3 facem cu I4 la celelalte mai speciale facem cu I8\n",
        "print(y_train)\n",
        "# trg = [[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,1.0,0.0],[0.0,0.0,0.0,1.0]]\n",
        "# y_train_target = []\n",
        "# for j in y_train:\n",
        "#   y_train_target.append(trg[j])\n",
        "# y_1 = np.array(y_train_target)\n",
        "# print(type(y_train_target[0][0]))\n",
        "\n",
        "y_test = y_train[60:]\n",
        "y_train = y_train[0:60]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOch9H7C62kl",
        "outputId": "a445a20e-a640-458d-a124-a93ecee5325d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 2, 0, 1, 0, 1, 2, 1, 0, 1, 1, 1, 0, 0, 2, 1, 2, 1, 1, 0, 1, 0, 3, 1, 1, 1, 0, 3, 2, 1, 1, 3, 1, 1, 1, 0, 1, 1, 3, 1, 3, 2, 1, 1, 1, 1, 1, 0, 1, 2, 1, 2, 2, 1, 1, 3, 1, 0, 1, 1, 2, 2, 0, 1, 0, 0, 3, 1, 1, 0, 1, 1, 1, 3, 2, 3, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
        "for classifier in classifiers:\n",
        "    classifier.fit(x_train,y_train)\n",
        "    print(f'The {classifier}  Accuracy  is {accuracy_score(y_test,classifier.predict(X)) }' )"
      ],
      "metadata": {
        "id": "jAWMPO-z5lKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Acesta are format de cod\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "umWrdTUVsN0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "vocab_size = 100000\n",
        "oov_token = \"<OOV>\"\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(x_train)"
      ],
      "metadata": {
        "id": "ks_uAhOiage_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "x_test_sequences = tokenizer.texts_to_sequences(x_test)\n"
      ],
      "metadata": {
        "id": "HvEbwiaEauB7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n"
      ],
      "metadata": {
        "id": "XsnPY8QIclYo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_length = 150530\n",
        "padding_type = \"post\"\n",
        "trunction_type=\"post\"\n",
        "x_train_padded = pad_sequences(x_train_sequences, padding=padding_type,\n",
        "                       truncating=trunction_type,maxlen = 1218)\n",
        "x_test_padded = pad_sequences(x_test_sequences,\n",
        "                               padding=padding_type, truncating=trunction_type,maxlen = 1218)"
      ],
      "metadata": {
        "id": "nZUw_ZO4bBAs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_test_data(category):\n",
        "  with open('drive/MyDrive/Licenta/reponses.json') as json_file:\n",
        "    responses = json.load(json_file)\n",
        "  y_train = responses[category]\n",
        "  \n",
        "  return y_train[0:60], y_train[60:]"
      ],
      "metadata": {
        "id": "fdo-7IOFuvhK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
        "def clasificatori(y_train,y_test):\n",
        "  all_r = []\n",
        "  for classifier in classifiers:\n",
        "      classifier.fit(x_train_padded,y_train)\n",
        "      all_r.append(classifier.predict(x_test_padded )) \n",
        "  return all_r\n",
        "      # print(f'The {classifier}  Accuracy  is {accuracy_score(y_test,classifier.predict(x_test_padded )) }' )"
      ],
      "metadata": {
        "id": "QNxmRil5Cx6P"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def get_categories(file):\n",
        "  with open(file) as f:\n",
        "    data = f.read()\n",
        "    questions = re.findall(\"[0-9]+\\.\\s*[a-zA-Z\\s-]+\\s*\\n\", data)\n",
        "  \n",
        "  for i, elem in enumerate(questions):\n",
        "    questions[i] = elem.split(\"\\n\")[0]\n",
        "    questions[i] = re.split(\"[0-9]+[\\.\\s]+\",questions[i])[1]\n",
        "\n",
        "  print(questions)\n",
        "  return questions"
      ],
      "metadata": {
        "id": "6HTK7vJWMvkj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_categories = get_categories(\"drive/MyDrive/chestionar.txt\")\n",
        "rez_0 = dict()\n",
        "rez_1 = dict()\n",
        "rez_2 = dict()\n",
        "rez_3 = dict()\n",
        "rez_4 = dict()\n",
        "rez_5 = dict()\n",
        "rez_6 = dict()\n",
        "rez_7 = dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZAnlRJKM315",
        "outputId": "d7485a12-2dbe-4f0e-8408-cee06ca8e1c0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sadness', 'Pessimism', 'Past Failure', 'Loss of Pleasure', 'Guilty Feelings', 'Punishment Feelings', 'Self-Dislike', 'Self-Criticalness', 'Suicidal Thoughts or Wishes', 'Crying', 'Agitation', 'Loss of Interest', 'Indecisiveness', 'Worthlessness', 'Loss of Energy', 'Changes in Sleeping Pattern', 'Irritability', 'Changes in Appetite', 'Concentration Difficulty', 'Tiredness or Fatigue', 'Loss of Interest in Sex']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def dump_to_json(file_name,data_dict_param):\n",
        "  with open(file_name, \"w\") as json_file:\n",
        "    json.dump(data_dict_param, json_file)\n"
      ],
      "metadata": {
        "id": "CyyM8TecPpGH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in all_categories:\n",
        "  y_train,y_test = get_train_test_data(c)\n",
        "  raspuns = clasificatori(y_train,y_test)\n",
        "  rez_0[c] = raspuns[0]\n",
        "  rez_1[c] = raspuns[1]\n",
        "  rez_2[c] = raspuns[2]\n",
        "  rez_3[c] = raspuns[3]\n",
        "  rez_4[c] = raspuns[4]\n",
        "  rez_5[c] = raspuns[5]\n",
        "  rez_6[c] = raspuns[6]\n",
        "  rez_7[c] = raspuns[7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EE_9ZQUM6hG",
        "outputId": "5283f652-b8d5-4da8-cb62-e0ade82f8252"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_ans(rezultate,path):\n",
        "  all_responses = dict()\n",
        "  for i in range(61,81):\n",
        "    all_responses[str(i)]= []\n",
        "  for c in all_categories:\n",
        "    r = rezultate[c]\n",
        "    print(r)\n",
        "    for index,o in enumerate(r):\n",
        "      all_responses[str(61+index)].append(o.item())\n",
        "    dump_to_json(\"drive/MyDrive/\"+path+\".json\",all_responses)\n"
      ],
      "metadata": {
        "id": "0TEImLy7PCIk"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_alg_responses = []\n",
        "def load_ok(path):\n",
        "  with open('drive/MyDrive/'+path+\".json\") as json_file:\n",
        "      data = json.load(json_file)\n",
        "  all_alg_responses.append(data)"
      ],
      "metadata": {
        "id": "MIuz3ZQ-bWbu"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rB2bzH8d6DN",
        "outputId": "f150f3c3-eabf-4673-eeb7-12a71c539a0f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'61': [1, 1, 0, 1, 0, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 4, 1, 5, 0, 1, 1], '62': [1, 1, 0, 1, 0, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 4, 1, 5, 0, 1, 1], '63': [1, 1, 0, 1, 0, 1, 3, 2, 2, 2, 1, 2, 1, 1, 2, 4, 1, 5, 0, 1, 1], '64': [1, 1, 0, 1, 0, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 4, 1, 5, 0, 1, 1], '65': [1, 1, 0, 1, 0, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 4, 1, 5, 0, 1, 1], '66': [1, 1, 0, 1, 0, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 4, 1, 5, 0, 1, 1], '67': [1, 1, 0, 1, 0, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 4, 1, 5, 0, 1, 1], '68': [1, 1, 0, 1, 0, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 4, 1, 5, 0, 1, 1], '69': [2, 3, 0, 1, 0, 1, 3, 0, 3, 2, 1, 2, 1, 1, 2, 4, 1, 5, 0, 2, 1], '70': [1, 1, 0, 1, 0, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 4, 1, 5, 0, 1, 1], '71': [1, 1, 0, 1, 0, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 4, 1, 5, 0, 1, 1], '72': [1, 1, 0, 1, 0, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 4, 1, 5, 0, 1, 1], '73': [2, 1, 0, 2, 0, 2, 3, 2, 3, 2, 1, 2, 0, 1, 2, 4, 1, 5, 0, 1, 1], '74': [1, 1, 0, 1, 0, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 4, 1, 5, 0, 1, 1], '75': [1, 1, 0, 1, 0, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 4, 1, 5, 0, 1, 1], '76': [1, 1, 0, 1, 0, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 4, 1, 5, 0, 1, 1], '77': [1, 1, 0, 1, 0, 2, 3, 0, 3, 2, 1, 2, 1, 1, 2, 4, 1, 5, 0, 1, 1], '78': [2, 2, 0, 1, 0, 2, 3, 0, 3, 3, 1, 2, 1, 1, 2, 4, 1, 5, 0, 2, 1], '79': [1, 1, 0, 1, 0, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 4, 1, 5, 0, 1, 1], '80': [1, 1, 2, 1, 0, 1, 3, 2, 3, 2, 1, 1, 0, 1, 2, 1, 1, 5, 0, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QYWc6Cd-d8gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index,o in enumerate(rez_0['Sadness']):\n",
        "  print(type(o.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdziMFSVcVT1",
        "outputId": "b06972d4-86c8-420b-8227-ef2833021c0b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_ok(\"Alg0\")\n",
        "load_ok(\"Alg1\")\n",
        "load_ok(\"Alg2\")\n",
        "load_ok(\"Alg3\")\n",
        "load_ok(\"Alg4\")\n",
        "load_ok(\"Alg5\")\n",
        "load_ok(\"Alg6\")\n",
        "load_ok(\"Alg7\")"
      ],
      "metadata": {
        "id": "X-EmWn8SPqlH"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_ans(rez_0,\"Alg0\")\n",
        "extract_ans(rez_1,\"Alg1\")\n",
        "extract_ans(rez_2,\"Alg2\")\n",
        "extract_ans(rez_3,\"Alg3\")\n",
        "extract_ans(rez_4,\"Alg4\")\n",
        "extract_ans(rez_5,\"Alg5\")\n",
        "extract_ans(rez_6,\"Alg6\")\n",
        "extract_ans(rez_7,\"Alg7\")"
      ],
      "metadata": {
        "id": "ckxL92RxedNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_correct_responses():\n",
        "  correct = dict()\n",
        "  for i in range(61,81):\n",
        "    correct[str(i)]= []\n",
        "  response  = \"drive/MyDrive/Licenta/response.txt\"\n",
        "  transform_resp = {\"1a\":1, \"1b\":2, \"2a\":3, \"2b\":4, \"3a\":5, \"3b\":6}\n",
        "  with open(response) as rsp:\n",
        "    data = rsp.readlines()[60:]\n",
        "    for index,line in enumerate(data):\n",
        "      res = line[:-1].split(\" \")[1:]\n",
        "      for j in res:\n",
        "        if len(j)<=1:\n",
        "          correct[str(61+index)].append(int(j))\n",
        "        else:\n",
        "            correct[str(61+index)].append(int(transform_resp[j]))\n",
        "    return correct\n",
        "\n",
        "corr = get_correct_responses()\n",
        "print(corr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h-VCHK_QEbA",
        "outputId": "7f30172d-7193-455a-fe74-038ae364c176"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'61': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0], '62': [1, 0, 1, 2, 1, 1, 1, 2, 0, 0, 1, 1, 2, 1, 0, 2, 0, 1, 1, 1, 0], '63': [1, 2, 3, 1, 1, 0, 3, 2, 1, 3, 2, 2, 3, 2, 2, 3, 2, 2, 1, 1, 1], '64': [2, 3, 2, 3, 1, 2, 3, 2, 1, 0, 3, 3, 3, 3, 3, 2, 1, 3, 2, 3, 2], '65': [2, 2, 3, 2, 3, 2, 2, 3, 1, 3, 2, 2, 3, 3, 3, 3, 2, 0, 1, 2, 0], '66': [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 2, 1, 0, 0, 1, 1, 1, 2, 2, 1, 2], '67': [1, 1, 3, 2, 3, 2, 3, 3, 2, 1, 1, 1, 3, 3, 1, 0, 1, 6, 2, 3, 1], '68': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0], '69': [0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 4, 1, 4, 2, 2, 1], '70': [3, 2, 2, 1, 2, 2, 2, 3, 1, 1, 2, 2, 2, 2, 2, 4, 2, 1, 2, 1, 0], '71': [1, 1, 0, 3, 1, 3, 2, 2, 0, 2, 2, 3, 1, 1, 3, 4, 0, 6, 2, 3, 3], '72': [1, 1, 0, 1, 0, 2, 1, 1, 0, 2, 1, 1, 3, 2, 0, 6, 0, 1, 2, 1, 0], '73': [0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 0, 0], '74': [1, 2, 1, 1, 0, 0, 0, 0, 1, 3, 1, 2, 1, 0, 2, 5, 2, 2, 2, 2, 2], '75': [1, 0, 1, 1, 3, 1, 3, 0, 1, 0, 1, 1, 2, 2, 2, 4, 1, 6, 3, 3, 1], '76': [1, 0, 0, 1, 2, 1, 2, 2, 1, 3, 1, 2, 1, 1, 1, 0, 2, 4, 1, 1, 2], '77': [3, 1, 0, 1, 0, 1, 1, 1, 1, 3, 1, 1, 0, 0, 0, 0, 1, 3, 1, 0, 0], '78': [2, 3, 2, 1, 1, 2, 2, 1, 2, 3, 2, 1, 1, 3, 1, 6, 1, 0, 2, 0, 3], '79': [3, 2, 2, 1, 1, 0, 3, 1, 2, 1, 1, 1, 0, 2, 2, 3, 1, 5, 2, 3, 1], '80': [1, 1, 2, 1, 1, 1, 3, 2, 0, 0, 1, 1, 0, 1, 1, 0, 1, 3, 2, 1, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all_alg_responses = [rez_0,rez_1,rez_2,rez_3,rez_4,rez_5,rez_6,rez_7]"
      ],
      "metadata": {
        "id": "ywcyy3q-Q0Li"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hr(l1, l2):\n",
        "  c = 0\n",
        "  for index, e in enumerate(l1):\n",
        "    if e == l2[index]:\n",
        "      c += 1\n",
        "  return c/21"
      ],
      "metadata": {
        "id": "MwWR1N86QFYT"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ahr(all_alg_responses):\n",
        "  ahr_list = []\n",
        "  for all_responses  in all_alg_responses:\n",
        "    s = 0\n",
        "    for i in range(61,81):\n",
        "      l1 = all_responses[str(i)]\n",
        "      l2 = corr[str(i)]\n",
        "      s += hr(l1,l2)\n",
        "    ahr_list.append(s/20)\n",
        "  return ahr_list"
      ],
      "metadata": {
        "id": "SWqRD7LJQIRW"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ahr(all_alg_responses)\n",
        "# print(all_alg_responses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjvzYPreQ-h4",
        "outputId": "722a772a-a574-4cdb-f65c-3c2f731ed12a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3095238095238095,\n",
              " 0.1928571428571429,\n",
              " 0.37142857142857144,\n",
              " 0.3261904761904762,\n",
              " 0.22142857142857145,\n",
              " 0.35714285714285715,\n",
              " 0.3666666666666666,\n",
              " 0.34523809523809523]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dict_transform(elem):\n",
        "  tr_dict={\"0\":0,\"1\":1,\"2\":1,\"3\":2,\"4\":2,\"5\":3,\"6\":3}\n",
        "  return tr_dict[str(elem)]"
      ],
      "metadata": {
        "id": "H0IE3L6ARLwA"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cr(l1,l2):\n",
        "  s = 0\n",
        "  # if mad == 7:\n",
        "  #   new_l1 = transform(l1)\n",
        "  #   new_l2 = transform(l2)\n",
        "  for index, e in enumerate(l1):\n",
        "    if index == 15 or index == 17:\n",
        "      mad = 7\n",
        "      e = dict_transform(e)\n",
        "      l2[index] = dict_transform(l2[index])\n",
        "    else:\n",
        "      mad = 4\n",
        "    ad = abs(e-l2[index]) \n",
        "    cr = (mad-ad)/mad\n",
        "    s = s+cr\n",
        "  return s/21"
      ],
      "metadata": {
        "id": "LX2xHoYFRPGV"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def acr(all_alg_responses):\n",
        "  acr_list = []\n",
        "  for all_responses  in all_alg_responses:\n",
        "    s = 0\n",
        "    for i in range(61,81):\n",
        "      l1 = all_responses[str(i)]\n",
        "      l2 = corr[str(i)]\n",
        "      s += cr(l1,l2)\n",
        "    acr_list.append(s/20)\n",
        "  return acr_list"
      ],
      "metadata": {
        "id": "_K0lDT3uRP_3"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acr(all_alg_responses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QImfy0ERRiZa",
        "outputId": "5af181c9-26f7-428b-f181-0fe8ba8d75f7"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7641156462585033,\n",
              " 0.6876700680272109,\n",
              " 0.8052721088435375,\n",
              " 0.7942176870748299,\n",
              " 0.6976190476190476,\n",
              " 0.8073979591836734,\n",
              " 0.8034863945578232,\n",
              " 0.8056122448979594]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dodl(l1,l2):\n",
        "  s1 = 0\n",
        "  s2 = 0\n",
        "  for index, e in enumerate(l1):\n",
        "    if index == 15 or index == 17:\n",
        "      e = dict_transform(e)\n",
        "      l2[index] = dict_transform(l2[index])\n",
        "    s1 += e\n",
        "    s2 += l2[index]\n",
        "  ad_overall = abs(s1-s2)\n",
        "  return (63-ad_overall)/63"
      ],
      "metadata": {
        "id": "ut7t48GxRkC4"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adodl(all_alg_responses):\n",
        "  adodl_list = []\n",
        "  for all_responses  in all_alg_responses:\n",
        "    s = 0\n",
        "    for i in range(61,81):\n",
        "      l1 = all_responses[str(i)]\n",
        "      l2 = corr[str(i)]\n",
        "      s += dodl(l1,l2)\n",
        "    adodl_list.append(s/20)\n",
        "  return adodl_list"
      ],
      "metadata": {
        "id": "HWs8fRjjRmqM"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adodl(all_alg_responses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA3QHjFOR5tc",
        "outputId": "304e2639-df13-42f4-9434-75366bd63e51"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8412698412698413,\n",
              " 0.815873015873016,\n",
              " 0.8134920634920636,\n",
              " 0.8380952380952381,\n",
              " 0.746825396825397,\n",
              " 0.8468253968253968,\n",
              " 0.8380952380952381,\n",
              " 0.8412698412698413]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dchr(l1,l2):\n",
        "  s1 = 0\n",
        "  s2 = 0\n",
        "  for index, e in enumerate(l1):\n",
        "    if index == 15 or index == 17:\n",
        "      e = dict_transform(e)\n",
        "      l2[index] = dict_transform(l2[index])\n",
        "    s1 += e\n",
        "    s2 += l2[index]\n",
        "  n1 = -1\n",
        "  n2 = -1\n",
        "  if s1>=0 and s1<=9:\n",
        "    n1 = 1\n",
        "  elif s1>=10 and s1<=18:\n",
        "    n1 = 2\n",
        "  elif s1>=19 and s1<=29:\n",
        "    n1 = 3\n",
        "  else:\n",
        "    n1 = 4\n",
        "  \n",
        "  if s2>=0 and s2<=9:\n",
        "    n2 = 1\n",
        "  elif s2>=10 and s2<=18:\n",
        "    n2 = 2\n",
        "  elif s2>=19 and s2<=29:\n",
        "    n2 = 3\n",
        "  else:\n",
        "    n2 = 4\n",
        "  \n",
        "  if n1 == n2:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "RfRt7c2iR7v5"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adchr(all_alg_responses):\n",
        "  adchr_list = []\n",
        "  for all_responses  in all_alg_responses:\n",
        "    s = 0\n",
        "    for i in range(61,81):\n",
        "      l1 = all_responses[str(i)]\n",
        "      l2 = corr[str(i)]\n",
        "      s += dchr(l1,l2)\n",
        "    adchr_list.append(s/20)\n",
        "  return adchr_list"
      ],
      "metadata": {
        "id": "28eSe0wBR-Zx"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adchr(all_alg_responses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNMOLkm5SO6A",
        "outputId": "aec94896-d70a-45cf-ec42-0e4cdba478f5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3, 0.3, 0.3, 0.25, 0.15, 0.25, 0.25, 0.25]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    }
  ]
}